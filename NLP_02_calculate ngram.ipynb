{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2. Implementing N-gram language modeling by NIKL corpus\n",
    "- Use only `NLRW1900000011.json` to calculate the probability of a sentence `특히 이에 대한 예산이 충분히 반영된다면 좋은 결과가 있을 것이라 생각한다` by unigram, bigram, trigram language model.\n",
    "  - Load newspaper corpus file and cleansing all sentences for processing.\n",
    "  - Build unigram, bigram, trigram language model.\n",
    "  - Cacluate the probability of a sentence by above model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = {PATH_TO_CORPUS_FILE}\n",
    "\n",
    "with open(path + 'NLRW1900000011.json', encoding='UTF8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "paragraphs = [] # extract contents only\n",
    "for document in data[\"document\"]:\n",
    "    for paragraph in document[\"paragraph\"]:\n",
    "        paragraphs.append(paragraph[\"form\"])\n",
    "\n",
    "\n",
    "sentences = [] # sentence tokenization\n",
    "for paragraph in paragraphs:\n",
    "    paragraph_splited = paragraph.split('.')\n",
    "    for sentence in paragraph_splited:\n",
    "        if sentence != '':\n",
    "            sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['새로운 희망 공유하고 새 출발하자', '기축년(己丑年) 새해다', '새로운 희망을 공유하고 새 출발을 다짐할 때다']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing special character\n",
    "- Remove special characters except Korean, English, numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s 기축년 새해다 /s>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cleansing_special(sentence):\n",
    "    sentence = re.sub(\"[‘, ’, ◇, ‘, ”,  ’, ', ·, \\“, ·, △, ●,  , ■, (, ), \\\", >>, `, /, -,∼,=,ㆍ<,>, .,?, !,【,】, …, ◆,%]\", \" \", sentence)\n",
    "    sentence = re.sub(\"[^가-힣0-9a-zA-Z\\\\s]\", \" \", sentence)\n",
    "    sentence = re.sub(\"\\s+\", \" \", sentence) # replace several spaces to one\n",
    "    \n",
    "    return f\"<s {sentence} /s>\"\n",
    "\n",
    "cleansing_special(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s 새로운 희망 공유하고 새 출발하자 /s>',\n",
       " '<s 기축년 새해다 /s>',\n",
       " '<s 새로운 희망을 공유하고 새 출발을 다짐할 때다 /s>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansed_sentences = []\n",
    "for paragraphs in sentences:\n",
    "    cleansed_sentences.append(cleansing_special(paragraphs))\n",
    "cleansed_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate probability of target sentece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Frequency of target sentence\n",
    "\\begin{equation}\n",
    "    P(< s 특히 이에 대한 예산이 충분히 반영된다면 좋은 결과가 있을 것이라 생각한다 /s >) \\approx \\frac{count(특히 이에 대한 예산이 충분히 반영된다면 좋은 결과가 있을 것이라 생각한다)}{count(모든 문장)}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/103568(=0.0000096554920)\n"
     ]
    }
   ],
   "source": [
    "target_sentence = '특히 이에 대한 예산이 충분히 반영된다면 좋은 결과가 있을 것이라 생각한다'\n",
    "\n",
    "target_cleansed = cleansing_special(target_sentence)\n",
    "prob_target = cleansed_sentences.count(target_cleansed) / len(cleansed_sentences)\n",
    "\n",
    "print('%d/%d(=%.13f)'%(cleansed_sentences.count(target_cleansed), len(cleansed_sentences), prob_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Unigram language model\n",
    "#### Unigram Model (k=1): $P(w_1 w_2 ... w_n) \\approx \\prod_{i} P(w_i)$\n",
    "\n",
    "\\begin{equation}\n",
    "    P(x_1) \\approx \\frac{count(x_1)}{count(N)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define ngram counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def ngram_counter(sentence_list, n):\n",
    "    \n",
    "    ngram = []\n",
    "    \n",
    "    for i, s in enumerate(sentence_list):\n",
    "        uni = s.split(\" \") # split by space\n",
    "        if n == 1:\n",
    "            ngram.extend(uni)\n",
    "        else:\n",
    "            ngram.extend(zip(*[uni[i:] for i in range(n)])) # zip(uni, uni[1:], uni[2:],.., uni[n-1,:])\n",
    "            \n",
    "    return Counter(ngram) # Counter returns {element : count} dict when recieving [element] list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram of target sentence:  ['<s', '특히', '이에', '대한', '예산이', '충분히', '반영된다면', '좋은', '결과가', '있을', '것이라', '생각한다', '/s>']\n",
      "Unigram Model with splitting by space: 7.805136e-35\n"
     ]
    }
   ],
   "source": [
    "unigram = ngram_counter(cleansed_sentences, 1)\n",
    "n_unigram = len(unigram) # count(N)\n",
    "\n",
    "target_unigram = target_cleansed.split(\" \")\n",
    "prob_target = 1\n",
    "for u in target_unigram:\n",
    "    # unigram[u]: count of u\n",
    "    prob_target *= unigram[u] / n_unigram\n",
    "\n",
    "print(\"Unigram of target sentence: \", target_unigram)\n",
    "print(\"Unigram Model with splitting by space: %e\"%(prob_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram language model\n",
    "### Bigram Model (k=2): $P(w_i|w_1 w_2 ... w_{i-1}) \\approx P(w_i|w_{i-1})$\n",
    "\n",
    "\\begin{equation}\n",
    "    P(x_2|x_1) \\approx \\frac{count(x_1,x_2)}{count(x_1)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram of target sentence:  [('<s', '특히'), ('특히', '이에'), ('이에', '대한'), ('대한', '예산이'), ('예산이', '충분히'), ('충분히', '반영된다면'), ('반영된다면', '좋은'), ('좋은', '결과가'), ('결과가', '있을'), ('있을', '것이라'), ('것이라', '생각한다'), ('생각한다', '/s>')]\n",
      "Bigram Model with splitting by space: 2.238120e-21\n"
     ]
    }
   ],
   "source": [
    "bigram= ngram_counter(cleansed_sentences, 2)\n",
    "\n",
    "target_unigram = target_cleansed.split(\" \")\n",
    "target_bigram = list(zip(target_unigram, target_unigram[1:])) \n",
    "\n",
    "prob_target = 1\n",
    "for u in target_bigram:\n",
    "    # count(x1) of bigram (x1, x2)\n",
    "    count_x1 = unigram[u[0]]\n",
    "    # count(x1, x2) by bigram\n",
    "    count_bigram = bigram[u]\n",
    "    prob_target *= count_bigram / count_x1\n",
    "    \n",
    "print(\"Bigram of target sentence: \", target_bigram)\n",
    "print(\"Bigram Model with splitting by space: %e\"%(prob_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram language model\n",
    "\n",
    "\\begin{equation}\n",
    "    P(x_3|x_1,x_2) \\approx \\frac{count(x_1,x_2,x_3)}{count(x_1,x_2)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram of target sentence:  [('<s', '특히', '이에'), ('특히', '이에', '대한'), ('이에', '대한', '예산이'), ('대한', '예산이', '충분히'), ('예산이', '충분히', '반영된다면'), ('충분히', '반영된다면', '좋은'), ('반영된다면', '좋은', '결과가'), ('좋은', '결과가', '있을'), ('결과가', '있을', '것이라'), ('있을', '것이라', '생각한다'), ('것이라', '생각한다', '/s>')]\n",
      "Trigram Model with splitting by space: 1.257710e-07\n"
     ]
    }
   ],
   "source": [
    "trigram = ngram_counter(cleansed_sentences, 3)\n",
    "target_trigram = list(zip(*[target_unigram[i:] for i in range(3)]))\n",
    "\n",
    "prob_target = 1\n",
    "for u in target_trigram:\n",
    "    count_x1x2 = bigram[(u[0], u[1])]\n",
    "    count_trigram = trigram[u]\n",
    "    prob_target *= count_trigram / count_x1x2\n",
    "    \n",
    "\n",
    "print(\"Trigram of target sentence: \", target_trigram)\n",
    "print(\"Trigram Model with splitting by space: %e\"%(prob_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
