{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3. Cross Entropy\n",
    "- Calculate cross entropy of each language model.\n",
    "- Use `NLRW1900000011.json` as training data, `NLRW1900000020.json` `WARW1900003745.json`as test data,\n",
    "  - `NLRW1900000020.json` is an newspaper article like training data, and `WARW1900003745.json` is a part of \"The Chronicles of Narnia\" so the genre of test dataset is far apart from each other.\n",
    "- Implement Unigram, Bigram, Trigram model from training data and calculate cross entropy using above test data.\n",
    "- Calcaulate difference (H(P,m) - H(p))\n",
    "- Tips\n",
    "  - In training corpus, entropy == cross entropy so the difference is 0.\n",
    "  - Unigram model calculate entropy as below.\n",
    "    - H(X) = -p(가) log2 p(가) -p(각) log2 p(각) ...- p(힣) log2 p(힣)\n",
    "  - p(X) : The probability of each word in test corpus.\n",
    "  - logp(m) : The probability of the model calculated from test corpus.\n",
    "  - Using ADD-1 smoothing to cope with n-gram which is not included in training corpus but exists in test corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "In Information Theory, entropy (denoted $H(X)$) of a random variable X is the expected log probabiltiy:\n",
    "\n",
    "\\begin{equation}\n",
    "    H(X) = - \\sum P(x)log_2 P(x)\n",
    "\\end{equation}\n",
    "\n",
    "and is a **measure of uncertainty.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Cross Entropy\n",
    "\n",
    "The cross entropy, H(p,m), of a true distribution **p** and a model distribution **m** is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    H(p,m) = - \\sum_{x} p(x) log_2 m(x)\n",
    "\\end{equation}\n",
    "\n",
    "The lower the cross entropy is the closer it is to the true distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Split paragraph with kss\n",
    "- If the length of splitted sentence is over 3, include it to sentence list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "\n",
    "def sentence_splitter(paragraph):\n",
    "    sentence_list = [f\"{s}\" for s in kss.split_sentences(paragraph) if len(s) > 3 ]\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Cleansing sentence\n",
    "- Cleansing all words except '가~힣'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleansing_special(sentence):\n",
    "    sentence = re.sub(\"[^가-힣]\", \" \", sentence)\n",
    "    sentence = re.sub(' +', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data Loading\n",
    "- Loading paragrap from json file's 'document' list and split it to sentence format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loadeing(filename):\n",
    "    sentences = []\n",
    "    doc = json.load(open(filename, 'rt', encoding='UTF8'))['document']\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        tmp = doc[i]['paragraph']\n",
    "\n",
    "        for j in range(len(tmp)):\n",
    "            sentences.extend(sentence_splitter(doc[i]['paragraph'][j]['form']))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = '{PATH}/NLRW1900000011.json' # train data set\n",
    "test1_filename = '{PATH}/NLRW1900000020.json' # test data set #1\n",
    "test2_filename = '{PATH}/WARW1900003745.json' # test data set #2\n",
    "\n",
    "data_list = ['train','test1','test2']\n",
    "\n",
    "sentences = {}\n",
    "\n",
    "sentences[data_list[0]] = data_loadeing(train_filename)\n",
    "\n",
    "sentences[data_list[1]] = data_loadeing(test1_filename)\n",
    "\n",
    "sentences[data_list[2]] = data_loadeing(test2_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나니아의 국경에서',\n",
       " '여기는 나니아의 국경이다.',\n",
       " '반 세기가 넘도록 우리 세계의 수많은 독자들이 울고 웃고 뒹굴며 모험을 즐긴 바로 그 나니아가 여러분의 코앞에 있다.',\n",
       " '이곳은 알려진 대로 판타지의 세계이다.',\n",
       " '진정한 판타지는 이상적인 세계에 대한 갈망을 일으키고 실제 세계에 새로운 차원의 깊이를 제공하는 것이라고, 작가는 말했다.',\n",
       " '나니아는 바로 그런 나라이다.',\n",
       " '게으름과 욕심, 슬픔과 절망, 소외감과 공포를 가지고 오면 희망과 용기, 사랑과 웃음, 자신감과 인내심을 값없이 드리겠다.',\n",
       " '여권으로 호기심만 갖추고 있다면 이 나라에 들어오는 조건은 별로 까다롭지 않다.',\n",
       " '그러나 여러분이 입국하기 전에 챙겨 오면 좋을 것을 지금 알려 드리겠다.',\n",
       " '첫째는 상상력이다.',\n",
       " '이런 종류의 판타지 나라를 만든 사람들이 다 그렇듯이, 《나니아 연대기》를 지은 C. S. 루이스는 상상력이 보통이 아닌 작가였다.',\n",
       " '병약했던 어린 시절, 북아일랜드의 수도인 벨파스트의 큰 집을 구석구석을 돌아다니며 환상의 날개를 펼쳤다.',\n",
       " '그 집은 동물의 왕국이 되기도 했고, 인도가 되기도 했다.',\n",
       " '책을 좋아했던 그는 세계 여러 나라의 신화와 전설, 선대 동화작가들의 작품 세계에 푹 빠져 살았다.',\n",
       " '그렇게 키워낸 상상력으로 이토록 생기 넘치는 나니아를 만들어낸 것이다.',\n",
       " '상상력이란, 보이는 것을 보이는 그대로, 주어진 것을 주어진 그대로 받아들이지 않고 자기만의 생각의 힘으로 변화시키고, 없는 것을 만들어내 덧붙여서, 새로운 세계를 창조하는 힘이다.',\n",
       " '그런 상상력으로 세워진 판타지 나라는 상상력 풍부한 여행객일수록 잘 이해하고 즐기는 법이니 상상력을 잊지 말고 챙기시길 바란다.',\n",
       " '둘째는 유머 감각이다.',\n",
       " '루이스는 유머 감각이 뛰어났고, 유머를 무엇보다 소중히 여기는 사람이었다.',\n",
       " '천국과 지옥의 차이를 유머가 있고 없고로 설명할 정도였으니.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[data_list[2]][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Cleansing sentences\n",
    "- sentences to cleansed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing_sentences(sentences):\n",
    "    cleansed_sentences = []\n",
    "    for s in sentences:\n",
    "        cleansed_sentences.append(cleansing_special(s))\n",
    "    return cleansed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_sentences = {}\n",
    "for data in data_list:\n",
    "    cleansed_sentences[data] = cleansing_sentences(sentences[data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking cleansed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Double-check for remained special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여기는 나니아의 국경이다.\n",
      "여기는 나니아의 국경이다\n",
      "\n",
      "반 세기가 넘도록 우리 세계의 수많은 독자들이 울고 웃고 뒹굴며 모험을 즐긴 바로 그 나니아가 여러분의 코앞에 있다.\n",
      "반 세기가 넘도록 우리 세계의 수많은 독자들이 울고 웃고 뒹굴며 모험을 즐긴 바로 그 나니아가 여러분의 코앞에 있다\n",
      "\n",
      "이곳은 알려진 대로 판타지의 세계이다.\n",
      "이곳은 알려진 대로 판타지의 세계이다\n",
      "\n",
      "진정한 판타지는 이상적인 세계에 대한 갈망을 일으키고 실제 세계에 새로운 차원의 깊이를 제공하는 것이라고, 작가는 말했다.\n",
      "진정한 판타지는 이상적인 세계에 대한 갈망을 일으키고 실제 세계에 새로운 차원의 깊이를 제공하는 것이라고 작가는 말했다\n",
      "\n",
      "나니아는 바로 그런 나라이다.\n",
      "나니아는 바로 그런 나라이다\n",
      "\n",
      "게으름과 욕심, 슬픔과 절망, 소외감과 공포를 가지고 오면 희망과 용기, 사랑과 웃음, 자신감과 인내심을 값없이 드리겠다.\n",
      "게으름과 욕심 슬픔과 절망 소외감과 공포를 가지고 오면 희망과 용기 사랑과 웃음 자신감과 인내심을 값없이 드리겠다\n",
      "\n",
      "여권으로 호기심만 갖추고 있다면 이 나라에 들어오는 조건은 별로 까다롭지 않다.\n",
      "여권으로 호기심만 갖추고 있다면 이 나라에 들어오는 조건은 별로 까다롭지 않다\n",
      "\n",
      "그러나 여러분이 입국하기 전에 챙겨 오면 좋을 것을 지금 알려 드리겠다.\n",
      "그러나 여러분이 입국하기 전에 챙겨 오면 좋을 것을 지금 알려 드리겠다\n",
      "\n",
      "첫째는 상상력이다.\n",
      "첫째는 상상력이다\n",
      "\n",
      "이런 종류의 판타지 나라를 만든 사람들이 다 그렇듯이, 《나니아 연대기》를 지은 C. S. 루이스는 상상력이 보통이 아닌 작가였다.\n",
      "이런 종류의 판타지 나라를 만든 사람들이 다 그렇듯이 나니아 연대기 를 지은 루이스는 상상력이 보통이 아닌 작가였다\n",
      "\n",
      "병약했던 어린 시절, 북아일랜드의 수도인 벨파스트의 큰 집을 구석구석을 돌아다니며 환상의 날개를 펼쳤다.\n",
      "병약했던 어린 시절 북아일랜드의 수도인 벨파스트의 큰 집을 구석구석을 돌아다니며 환상의 날개를 펼쳤다\n",
      "\n",
      "그 집은 동물의 왕국이 되기도 했고, 인도가 되기도 했다.\n",
      "그 집은 동물의 왕국이 되기도 했고 인도가 되기도 했다\n",
      "\n",
      "책을 좋아했던 그는 세계 여러 나라의 신화와 전설, 선대 동화작가들의 작품 세계에 푹 빠져 살았다.\n",
      "책을 좋아했던 그는 세계 여러 나라의 신화와 전설 선대 동화작가들의 작품 세계에 푹 빠져 살았다\n",
      "\n",
      "그렇게 키워낸 상상력으로 이토록 생기 넘치는 나니아를 만들어낸 것이다.\n",
      "그렇게 키워낸 상상력으로 이토록 생기 넘치는 나니아를 만들어낸 것이다\n",
      "\n",
      "상상력이란, 보이는 것을 보이는 그대로, 주어진 것을 주어진 그대로 받아들이지 않고 자기만의 생각의 힘으로 변화시키고, 없는 것을 만들어내 덧붙여서, 새로운 세계를 창조하는 힘이다.\n",
      "상상력이란 보이는 것을 보이는 그대로 주어진 것을 주어진 그대로 받아들이지 않고 자기만의 생각의 힘으로 변화시키고 없는 것을 만들어내 덧붙여서 새로운 세계를 창조하는 힘이다\n",
      "\n",
      "그런 상상력으로 세워진 판타지 나라는 상상력 풍부한 여행객일수록 잘 이해하고 즐기는 법이니 상상력을 잊지 말고 챙기시길 바란다.\n",
      "그런 상상력으로 세워진 판타지 나라는 상상력 풍부한 여행객일수록 잘 이해하고 즐기는 법이니 상상력을 잊지 말고 챙기시길 바란다\n",
      "\n",
      "둘째는 유머 감각이다.\n",
      "둘째는 유머 감각이다\n",
      "\n",
      "루이스는 유머 감각이 뛰어났고, 유머를 무엇보다 소중히 여기는 사람이었다.\n",
      "루이스는 유머 감각이 뛰어났고 유머를 무엇보다 소중히 여기는 사람이었다\n",
      "\n",
      "천국과 지옥의 차이를 유머가 있고 없고로 설명할 정도였으니.\n",
      "천국과 지옥의 차이를 유머가 있고 없고로 설명할 정도였으니\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,20):\n",
    "    print(sentences['test2'][i])\n",
    "    print(cleansed_sentences['test2'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define ngram counter function & Excution\n",
    "- Using `zip` function to implement n-gram.\n",
    "- Using `Counter` function from `collections` package to count n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_counter(sentence_list, n):\n",
    "    ngram = []\n",
    "    \n",
    "    for i, s in enumerate(sentence_list):\n",
    "        if n == 1:\n",
    "            ngram.extend(s)\n",
    "        else:\n",
    "            ngram.extend(zip(*[s[i:] for i in range(n)]))\n",
    "            \n",
    "    return Counter(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_count={} # Save the number of ngram in dict \n",
    "\n",
    "for data in data_list:\n",
    "    ngram_count[data]={}\n",
    "    for i in range(4):\n",
    "        ngram_count[data][i]=ngram_counter(cleansed_sentences[data], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2.Smoothing (Add-1) implementation\n",
    "- Add 1 to every n-gram from training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1,4):\n",
    "    ngram_count['train'][n] = dict(zip(ngram_count['train'][n].keys(),map(lambda x:x[1]+1,\n",
    "                                                                          ngram_count['train'][n].items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Process n-grams which is not in training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_list[1:]:\n",
    "    for n in range(1,4):\n",
    "        for ng in ngram_count[data][n].keys():\n",
    "            if ng not in ngram_count['train'][n]:\n",
    "                ngram_count['train'][n][ng]=1  # Initialize to 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Count all n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_count_total={}\n",
    "for data in data_list:\n",
    "    ngram_count_total[data] = {}\n",
    "    for n in range(1,4):\n",
    "        ngram_count_total[data][n]=sum(ngram_count[data][n].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate the probability of each n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_ngram = {}\n",
    "\n",
    "for data in data_list:\n",
    "    prob_ngram[data]={}\n",
    "    for n in range(1,4):\n",
    "        ngram=[]\n",
    "        prob_ngram[data][n] = {}\n",
    "        for ng in ngram_count[data][n].keys():\n",
    "            prob_ngram[data][n][ng] = ngram_count[data][n][ng] / ngram_count_total[data][n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Entropy, Cross-entropy, Difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>n-gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NLRW1900000011.json</td>\n",
       "      <td>unigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>bigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>trigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLRW1900000020.json</td>\n",
       "      <td>unigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>bigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>trigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WARW1900003745.json</td>\n",
       "      <td>unigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>bigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>trigram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File Name   n-gram\n",
       "0  NLRW1900000011.json  unigram\n",
       "1                        bigram\n",
       "2                       trigram\n",
       "3  NLRW1900000020.json  unigram\n",
       "4                        bigram\n",
       "5                       trigram\n",
       "6  WARW1900003745.json  unigram\n",
       "7                        bigram\n",
       "8                       trigram"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = {'File Name': ['NLRW1900000011.json', '', '', 'NLRW1900000020.json','', '', 'WARW1900003745.json', '', ''],\n",
    "            'n-gram': ['unigram','bigram','trigram','unigram','bigram','trigram','unigram','bigram','trigram']}\n",
    "\n",
    "result = pd.DataFrame(raw_data)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Entropy \n",
    "- Save results to `result['Entropy']`\n",
    "$$H(p) = -\\sum_{i=1}^{n} p(x_i){log} p(x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entropy=[]\n",
    "for data in data_list:\n",
    "    for n in range(1,4):\n",
    "        entropy.append(-sum(p*np.log2(p) for p in prob_ngram[data][n].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result['Entropy']=entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Cross-entropy \n",
    "- Save results to `result['Cross-entropy']`\n",
    "$$H(p,q) = -\\sum_{i=1}^{n} p(x_i)logq(x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy=[]\n",
    "for data in data_list:\n",
    "    for n in range(1,4):\n",
    "         cross_entropy.append(-sum(prob_ngram[data][n][ng]*np.log2(prob_ngram['train'][n][ng])\n",
    "                                   for ng in prob_ngram[data][n].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result['Cross-entropy']=cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>n-gram</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Cross-entropy</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NLRW1900000011.json</td>\n",
       "      <td>unigram</td>\n",
       "      <td>6.875718</td>\n",
       "      <td>6.875718</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>bigram</td>\n",
       "      <td>11.854136</td>\n",
       "      <td>11.854136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>trigram</td>\n",
       "      <td>15.790079</td>\n",
       "      <td>15.790079</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLRW1900000020.json</td>\n",
       "      <td>unigram</td>\n",
       "      <td>6.912301</td>\n",
       "      <td>6.949638</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>bigram</td>\n",
       "      <td>11.672032</td>\n",
       "      <td>11.956467</td>\n",
       "      <td>0.284435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>trigram</td>\n",
       "      <td>15.071285</td>\n",
       "      <td>15.855163</td>\n",
       "      <td>0.783878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WARW1900003745.json</td>\n",
       "      <td>unigram</td>\n",
       "      <td>6.777204</td>\n",
       "      <td>7.268261</td>\n",
       "      <td>0.491056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>bigram</td>\n",
       "      <td>11.117389</td>\n",
       "      <td>12.806630</td>\n",
       "      <td>1.689241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>trigram</td>\n",
       "      <td>14.275727</td>\n",
       "      <td>17.162749</td>\n",
       "      <td>2.887022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File Name   n-gram    Entropy  Cross-entropy  Difference\n",
       "0  NLRW1900000011.json  unigram   6.875718       6.875718    0.000000\n",
       "1                        bigram  11.854136      11.854136    0.000000\n",
       "2                       trigram  15.790079      15.790079    0.000000\n",
       "3  NLRW1900000020.json  unigram   6.912301       6.949638    0.037337\n",
       "4                        bigram  11.672032      11.956467    0.284435\n",
       "5                       trigram  15.071285      15.855163    0.783878\n",
       "6  WARW1900003745.json  unigram   6.777204       7.268261    0.491056\n",
       "7                        bigram  11.117389      12.806630    1.689241\n",
       "8                       trigram  14.275727      17.162749    2.887022"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['Difference'] = result['Cross-entropy'] - result['Entropy']\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
