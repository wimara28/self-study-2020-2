{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final. Korean to English Translation\n",
    "\n",
    "- Implement Korean to English translator using (Packed) Encoder-decoder, Convolution and Transformers.\n",
    "  - Checking PPL, BLEU score.\n",
    "  - Try to translate `unk` word using romanizer [link](https://github.com/osori/korean-romanizer).\n",
    "- Reference site [link](https://github.com/bentrevett/pytorch-seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "- `ko-en-en.parse.syn` Englinsh version of parsed file with POS \n",
    "```\n",
    "(ROOT (S (NP (NNP Flight) (NNP 007)) (VP (MD will) (VP (VB stay) (PP (IN on) (NP (NP (DT the) (NN ground)) (PP (IN for) (NP (CD one) (NN hour))))))) (. .)))\n",
    "```\n",
    "- `ko-en-ko.parse.syn` Corresponding Korean version file woth POS and morpheme.\n",
    "```\n",
    "</id 1>\n",
    "</sent 1>\n",
    "1       2       NP      777/SN\n",
    "2       6       NP_SBJ  항공편/NNG|은/JX\n",
    "3       4       NP      1/SN|시간/NNG\n",
    "4       6       NP_AJT  동안/NNG\n",
    "5       6       NP_AJT  지상/NNG|에/JKB\n",
    "6       7       VP      머물/VV|게/EC\n",
    "7       0       VP      되/VV|ㅂ니다/EF|./SF\n",
    "</sent>\n",
    "</id>\n",
    "```\n",
    "- total 330,974\n",
    "- Processing two files to Korean-Enlish parallel data and using it as train & validation set.\n",
    "- Import nltk module to use parsed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "- Extract contents only.\n",
    "- Divide by 8:1:1 as train, valid, test set.\n",
    "- Process them as a csv format and apply Torchtext module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3VM-yByTW1N",
    "outputId": "beb7f131-9334-4ceb-f0c1-cfd162e82261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Total examples: 330974\n",
      "Number of training examples: 265109\n",
      "Number of validation examples: 32767\n",
      "Number of testing examples: 33098\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Total examples: {len(train_data.examples)+len(valid_data.examples)+len(test_data.examples)}\")\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizer\n",
    "- Since it was a tokenizer sentence, only space was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocab\n",
    "- Insert words that appear more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ko) vocabulary: 16263\n",
      "Unique tokens in target (en) vocabulary: 16059\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (ko) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "### Transformers \n",
    "\n",
    "|Model|Best Val. Loss|Val. PPL |Test Loss|Test PPL|BLEU Score|\n",
    "|------------------------|----|----|----|----|----|\n",
    "|(Packed) Encoder-Decoder|2.26|9.57|2.26|9.60|46.75|\n",
    "|Convolutional Seq to Seq|1.50|4.48|2.31|10.12|37.09|\n",
    "|Transformers|0.88|2.41|<span style=\"color:blue\">0.88</span>|<span style=\"color:blue\">2.42</span>|<span style=\"color:blue\">55.86</span>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of Hyperparmeters Tuning\n",
    "\n",
    "|Batch size|Learning Rate|Drop Out| ENC/DEC Layers|Best Val. Loss|Val. PPL |Test Loss|Test PPL|BLEU Score|Note|\n",
    "|----------|-------------|--------|---------------|--------------|---------|---------|--------|----------|----|\n",
    "|128|0.0005|0.1|3|0.94|2.57|0.95|2.58|51.20|Batch Size 비교|\n",
    "|256|0.0005|0.1|3|0.91|2.49|0.92|2.50|-|\n",
    "|64|0.0005|0.1|3|0.95|2.57|0.95|2.59|-|\n",
    "|----------|-------------|--------|---------------|--------------|---------|---------|--------|----------|----|\n",
    "|256|0.0001|0.1|3|1.08|2.93|1.07|2.93|-|Learning Rate 비교|\n",
    "|256|0.0005|0.1|3|0.91|2.49|0.92|2.50|-|\n",
    "|256|0.0008|0.1|3|0.97|2.63|0.97|2.64|-|\n",
    "|256|0.0010|0.1|3|0.95|2.58|0.95|2.58|-|\n",
    "|256|0.00001|0.1|3|2.10|8.19|2.10|8.18|-|\n",
    "|----------|-------------|--------|---------------|--------------|---------|---------|--------|----------|----|\n",
    "|256|0.0005|0.1|3|0.91|2.49|0.92|2.50|-|Drop Out 비교|\n",
    "|256|0.0005|0.5|3|0.92|2.50|0.92|2.51|-||\n",
    "|256|0.0005|0.7|3|0.92|2.52|0.93|2.52|-||\n",
    "|----------|-------------|--------|---------------|--------------|---------|---------|--------|----------|----|\n",
    "|256|0.0005|0.1|3|0.91|2.49|0.92|2.50|-|Epoch 10|\n",
    "|<span style=\"color:blue\">256</span>|0.0005|0.1|3|0.88|2.41|<span style=\"color:blue\">0.88</span>|<span style=\"color:blue\">2.42</span>|<span style=\"color:blue\">55.86</span>|Epoch 20|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Result of Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare senteces\n",
    "\n",
    "|Model|Sentence|\n",
    "|------|----------------------------------------------------------------------------|\n",
    "|-|그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?|\n",
    "|(Packed) Encoder-Decoder|that size for the invoice for me . i think you can . how you you for this for the '\\<'unk'\\>' for the '\\<'unk'\\>' ?|\n",
    "|Convolutional|how much is it in a little ?|\n",
    "    |<span style=\"color:blue\">Transformers</span>|<span style=\"color:blue\">show me that pants are looking at all . how much can i buy this one ?</span>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Packed) Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .</td>\n",
       "      <td>all liquids , gels aerosols aerosols , gels , aerosols in the , , and aerosols in the upper container . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?</td>\n",
       "      <td>i 'm sorry , &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; . &lt;unk&gt; . &lt;unk&gt; . &lt;unk&gt; . &lt;unk&gt; . wo n't you take a &lt;unk&gt; &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요</td>\n",
       "      <td>it 's why not a a big reason . wo n't you have to pay it &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?</td>\n",
       "      <td>i have n't leak out the &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; . &lt;unk&gt; &lt;unk&gt; . &lt;unk&gt; . &lt;unk&gt; . . . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .</td>\n",
       "      <td>the &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>변기 가 막히 었 습니다 .</td>\n",
       "      <td>the &lt;unk&gt; &lt;unk&gt; the the city i was n't . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?</td>\n",
       "      <td>that size for the invoice for me . i think you can . how you you for this for the &lt;unk&gt; for the &lt;unk&gt; ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .</td>\n",
       "      <td>it was the to the the &lt;unk&gt; journey was &lt;unk&gt; and &lt;unk&gt; journey was &lt;unk&gt; and &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다</td>\n",
       "      <td>it is &lt;unk&gt; from the &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; after &lt;unk&gt; and &lt;unk&gt; after the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .</td>\n",
       "      <td>the athletic always is the the &lt;unk&gt; and &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .</td>\n",
       "      <td>this &lt;unk&gt; &lt;unk&gt; this &lt;unk&gt; &lt;unk&gt; , and to extend the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .</td>\n",
       "      <td>cif varies of &lt;unk&gt; &lt;unk&gt; , the &lt;unk&gt; are both acceptable to &lt;unk&gt; . . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .</td>\n",
       "      <td>the author &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; , &lt;unk&gt; &lt;unk&gt; functions . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .</td>\n",
       "      <td>this laws headquarters headquarters headquarters &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다</td>\n",
       "      <td>have this been been performance many &lt;unk&gt; many &lt;unk&gt; performance or &lt;unk&gt; the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Korean  \\\n",
       "0                                  모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .   \n",
       "1                 미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?   \n",
       "2                                            은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요   \n",
       "3                         아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?   \n",
       "4                            부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .   \n",
       "5                                                                           변기 가 막히 었 습니다 .   \n",
       "6                                         그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?   \n",
       "7                                              비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .   \n",
       "8                                                    속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다   \n",
       "9                                                      문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .   \n",
       "10                                                        이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .   \n",
       "11                                                       이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .   \n",
       "12                     통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .   \n",
       "13  이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .   \n",
       "14                                                             요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다    \n",
       "\n",
       "                                                                                                                                             English  \n",
       "0                                      all liquids , gels aerosols aerosols , gels , aerosols in the , , and aerosols in the upper container . <eos>  \n",
       "1   i 'm sorry , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <unk> . <unk> . <unk> . <unk> . wo n't you take a <unk> <unk> . <eos>  \n",
       "2                                                                             it 's why not a a big reason . wo n't you have to pay it <unk> . <eos>  \n",
       "3                                                                i have n't leak out the <unk> <unk> <unk> . <unk> <unk> . <unk> . <unk> . . . <eos>  \n",
       "4                                  the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <eos>  \n",
       "5                                                                                                     the <unk> <unk> the the city i was n't . <eos>  \n",
       "6                                      that size for the invoice for me . i think you can . how you you for this for the <unk> for the <unk> ? <eos>  \n",
       "7                                                        it was the to the the <unk> journey was <unk> and <unk> journey was <unk> and <unk> . <eos>  \n",
       "8                                                         it is <unk> from the <unk> <unk> <unk> <unk> after <unk> and <unk> after the <unk> . <eos>  \n",
       "9                                                                             the athletic always is the the <unk> and <unk> <unk> and <unk> . <eos>  \n",
       "10                                                                               this <unk> <unk> this <unk> <unk> , and to extend the <unk> . <eos>  \n",
       "11                                                                      cif varies of <unk> <unk> , the <unk> are both acceptable to <unk> . . <eos>  \n",
       "12                                                  the author <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> <unk> functions . <eos>  \n",
       "13            this laws headquarters headquarters headquarters <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <eos>  \n",
       "14                                                      have this been been performance many <unk> many <unk> performance or <unk> the <unk> . <eos>  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "hGj-fPAS-JpZ",
    "outputId": "79eff127-53bf-4c8b-a538-33c25d460895",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .</td>\n",
       "      <td>i 'm a bag , gels , gels , gels , gels , gels , and liquids . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?</td>\n",
       "      <td>can you give me a &lt;unk&gt; for the &lt;unk&gt; to &lt;unk&gt; , but i 'm afraid i 'm going to get a &lt;unk&gt; to go to &lt;unk&gt; , but i 'm going to get a &lt;unk&gt; for the &lt;unk&gt; to &lt;unk&gt; to get a &lt;unk&gt; , but i 'm going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요</td>\n",
       "      <td>i need money . the &lt;unk&gt; is not too good . the bank is not too far to get a full . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?</td>\n",
       "      <td>are you going to have to have to have to have to have to have to have a form . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .</td>\n",
       "      <td>&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; to &lt;unk&gt; &lt;unk&gt; to &lt;unk&gt; &lt;unk&gt; to &lt;unk&gt; &lt;unk&gt; to &lt;unk&gt; &lt;unk&gt; from &lt;unk&gt; &lt;unk&gt; from &lt;unk&gt; &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>변기 가 막히 었 습니다 .</td>\n",
       "      <td>it 's a traffic jam . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?</td>\n",
       "      <td>how much is it in a little ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .</td>\n",
       "      <td>i 'd like to have a department store . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다</td>\n",
       "      <td>i do n't feel good to be a good time to go in the morning . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .</td>\n",
       "      <td>i 'm afraid the profit is &lt;unk&gt; from the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .</td>\n",
       "      <td>i 'll take a few time to go to a few time to go for a few time to go to a few time to go to a few time to take a few time to go to a few time to go to a few time to take a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .</td>\n",
       "      <td>i have a full name of the job . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .</td>\n",
       "      <td>the &lt;unk&gt; &lt;unk&gt; the &lt;unk&gt; &lt;unk&gt; the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .</td>\n",
       "      <td>i need to have a &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다</td>\n",
       "      <td>the program is &lt;unk&gt; for the end . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Korean                                                                                                                                                                                           English\n",
       "0                                  모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .                                                                                                                               i 'm a bag , gels , gels , gels , gels , gels , and liquids . <eos>\n",
       "1                 미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?  can you give me a <unk> for the <unk> to <unk> , but i 'm afraid i 'm going to get a <unk> to go to <unk> , but i 'm going to get a <unk> for the <unk> to <unk> to get a <unk> , but i 'm going\n",
       "2                                            은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요                                                                                                          i need money . the <unk> is not too good . the bank is not too far to get a full . <eos>\n",
       "3                         아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?                                                                                                              are you going to have to have to have to have to have to have to have a form . <eos>\n",
       "4                            부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .                                                                     <unk> <unk> <unk> <unk> to <unk> <unk> to <unk> <unk> to <unk> <unk> to <unk> <unk> from <unk> <unk> from <unk> <unk> . <eos>\n",
       "5                                                                           변기 가 막히 었 습니다 .                                                                                                                                                                       it 's a traffic jam . <eos>\n",
       "6                                         그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?                                                                                                                                                                how much is it in a little ? <eos>\n",
       "7                                              비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .                                                                                                                                                      i 'd like to have a department store . <eos>\n",
       "8                                                    속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다                                                                                                                                 i do n't feel good to be a good time to go in the morning . <eos>\n",
       "9                                                      문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .                                                                                                                                            i 'm afraid the profit is <unk> from the <unk> . <eos>\n",
       "10                                                        이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .                       i 'll take a few time to go to a few time to go for a few time to go to a few time to go to a few time to take a few time to go to a few time to go to a few time to take a\n",
       "11                                                       이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .                                                                                                                                                             i have a full name of the job . <eos>\n",
       "12                     통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .                                                      the <unk> <unk> the <unk> <unk> the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> . <eos>\n",
       "13  이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .                                                                                                   i need to have a <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> . <eos>\n",
       "14                                                             요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다                                                                                                                                                           the program is <unk> for the end . <eos>"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "executionInfo": {
     "elapsed": 2379,
     "status": "ok",
     "timestamp": 1607926027116,
     "user": {
      "displayName": "rim jj",
      "photoUrl": "",
      "userId": "15077449486487391682"
     },
     "user_tz": -540
    },
    "id": "bZgb-pIMc7gZ",
    "outputId": "693f697f-596f-44e6-e402-d28db22a76aa",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .</td>\n",
       "      <td>all liquids , gels and aerosols must be placed in a single , zip-top , clear plastic bag . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?</td>\n",
       "      <td>excuse me . my child is laundered clothes because my children 's collection library . do n't you have credit ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요</td>\n",
       "      <td>i 'm afraid it 's too far behind in the bank . you need cash payment in cash advance . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?</td>\n",
       "      <td>we may have lost the baggage , so we 'd like to make a lost baggage report . would you come with me to the office ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .</td>\n",
       "      <td>&lt;unk&gt; governor &lt;unk&gt; , &lt;unk&gt; and i will be promoted to daegu &lt;unk&gt; between electric 2000 . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>변기 가 막히 었 습니다 .</td>\n",
       "      <td>the toilet is stopped up . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?</td>\n",
       "      <td>show me that pants are looking at all . how much can i buy this one ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .</td>\n",
       "      <td>i 'd like to go to the duta and take care of the department store . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다</td>\n",
       "      <td>i ca n't wash my hands when i feel bad . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .</td>\n",
       "      <td>the president told me to build a group 's &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .</td>\n",
       "      <td>give me some time to look for this . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .</td>\n",
       "      <td>this &lt;unk&gt; &lt;unk&gt; the handle of &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .</td>\n",
       "      <td>they &lt;unk&gt; the grass &lt;unk&gt; , which changes to &lt;unk&gt; the other &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; 's recent miles . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .</td>\n",
       "      <td>please help us to make a &lt;unk&gt; to investigate it because we intend to fill in this event . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다</td>\n",
       "      <td>the record shows that video program is &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Korean                                                                                                                    English\n",
       "0                                  모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .                           all liquids , gels and aerosols must be placed in a single , zip-top , clear plastic bag . <eos>\n",
       "1                 미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?       excuse me . my child is laundered clothes because my children 's collection library . do n't you have credit ? <eos>\n",
       "2                                            은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요                               i 'm afraid it 's too far behind in the bank . you need cash payment in cash advance . <eos>\n",
       "3                         아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?  we may have lost the baggage , so we 'd like to make a lost baggage report . would you come with me to the office ? <eos>\n",
       "4                            부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .                           <unk> governor <unk> , <unk> and i will be promoted to daegu <unk> between electric 2000 . <eos>\n",
       "5                                                                           변기 가 막히 었 습니다 .                                                                                           the toilet is stopped up . <eos>\n",
       "6                                         그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?                                                show me that pants are looking at all . how much can i buy this one ? <eos>\n",
       "7                                              비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .                                                  i 'd like to go to the duta and take care of the department store . <eos>\n",
       "8                                                    속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다                                                                             i ca n't wash my hands when i feel bad . <eos>\n",
       "9                                                      문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .                                                                    the president told me to build a group 's <unk> . <eos>\n",
       "10                                                        이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .                                                                                 give me some time to look for this . <eos>\n",
       "11                                                       이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .                                                                               this <unk> <unk> the handle of <unk> . <eos>\n",
       "12                     통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .        they <unk> the grass <unk> , which changes to <unk> the other <unk> <unk> <unk> <unk> <unk> 's recent miles . <eos>\n",
       "13  이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .                           please help us to make a <unk> to investigate it because we intend to fill in this event . <eos>\n",
       "14                                                             요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다                                                                        the record shows that video program is <unk> . <eos>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further Study\n",
    "### Romanizer\n",
    "1) To process `<unk>`, replace Korean to Roman.\n",
    "2) Except word that not included in Roman\n",
    "\n",
    "**Compare Results**\n",
    "\n",
    "|Type|Sentence|\n",
    "|------|----------------------------------------------------------------------------|\n",
    "|-|통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .|\n",
    "|기존|they '\\<'unk'\\>' the grass '\\<'unk'\\>' , which changes to '\\<'unk'\\>' the other '\\<'unk'\\>' '\\<'unk'\\>' '\\<'unk'\\>' '\\<'unk'\\>' '\\<'unk'\\>' 's recent miles .|\n",
    "|Romanize|they <span style=\"color:blue\">useung</span> the grass <span style=\"color:blue\">reul</span> , which changes to <span style=\"color:blue\">naseongbeom</span> the other <span style=\"color:blue\">jinchul iraneun tto dareun kkum</span> 's recent miles .|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "executionInfo": {
     "elapsed": 2379,
     "status": "ok",
     "timestamp": 1607926027116,
     "user": {
      "displayName": "rim jj",
      "photoUrl": "",
      "userId": "15077449486487391682"
     },
     "user_tz": -540
    },
    "id": "bZgb-pIMc7gZ",
    "outputId": "693f697f-596f-44e6-e402-d28db22a76aa",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .</td>\n",
       "      <td>all liquids , gels and aerosols must be placed in a single , zip-top , clear plastic bag . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?</td>\n",
       "      <td>excuse me . my child is laundered clothes because my children 's collection library . do n't you have credit ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요</td>\n",
       "      <td>i 'm afraid it 's too far behind in the bank . you need cash payment in cash advance . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?</td>\n",
       "      <td>we may have lost the baggage , so we 'd like to make a lost baggage report . would you come with me to the office ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .</td>\n",
       "      <td>&lt;unk&gt; governor &lt;unk&gt; , &lt;unk&gt; and i will be promoted to daegu &lt;unk&gt; between electric 2000 . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>변기 가 막히 었 습니다 .</td>\n",
       "      <td>the toilet is stopped up . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?</td>\n",
       "      <td>show me that pants are looking at all . how much can i buy this one ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .</td>\n",
       "      <td>i 'd like to go to the duta and take care of the department store . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다</td>\n",
       "      <td>i ca n't wash my hands when i feel bad . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .</td>\n",
       "      <td>the president told me to build a group 's &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .</td>\n",
       "      <td>give me some time to look for this . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .</td>\n",
       "      <td>this &lt;unk&gt; &lt;unk&gt; the handle of &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .</td>\n",
       "      <td>they &lt;unk&gt; the grass &lt;unk&gt; , which changes to &lt;unk&gt; the other &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; 's recent miles . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .</td>\n",
       "      <td>please help us to make a &lt;unk&gt; to investigate it because we intend to fill in this event . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다</td>\n",
       "      <td>the record shows that video program is &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Korean                                                                                                                    English\n",
       "0                                  모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .                           all liquids , gels and aerosols must be placed in a single , zip-top , clear plastic bag . <eos>\n",
       "1                 미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?       excuse me . my child is laundered clothes because my children 's collection library . do n't you have credit ? <eos>\n",
       "2                                            은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요                               i 'm afraid it 's too far behind in the bank . you need cash payment in cash advance . <eos>\n",
       "3                         아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?  we may have lost the baggage , so we 'd like to make a lost baggage report . would you come with me to the office ? <eos>\n",
       "4                            부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .                           <unk> governor <unk> , <unk> and i will be promoted to daegu <unk> between electric 2000 . <eos>\n",
       "5                                                                           변기 가 막히 었 습니다 .                                                                                           the toilet is stopped up . <eos>\n",
       "6                                         그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?                                                show me that pants are looking at all . how much can i buy this one ? <eos>\n",
       "7                                              비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .                                                  i 'd like to go to the duta and take care of the department store . <eos>\n",
       "8                                                    속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다                                                                             i ca n't wash my hands when i feel bad . <eos>\n",
       "9                                                      문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .                                                                    the president told me to build a group 's <unk> . <eos>\n",
       "10                                                        이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .                                                                                 give me some time to look for this . <eos>\n",
       "11                                                       이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .                                                                               this <unk> <unk> the handle of <unk> . <eos>\n",
       "12                     통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .        they <unk> the grass <unk> , which changes to <unk> the other <unk> <unk> <unk> <unk> <unk> 's recent miles . <eos>\n",
       "13  이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .                           please help us to make a <unk> to investigate it because we intend to fill in this event . <eos>\n",
       "14                                                             요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다                                                                        the record shows that video program is <unk> . <eos>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "WiGycdqe3VNa",
    "outputId": "5176f5ed-173a-459a-93e9-ef1ce3d1a207",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .</td>\n",
       "      <td>all liquids , gels and aerosols must be placed in a single , zip-top , clear plastic bag . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?</td>\n",
       "      <td>excuse me . my child is laundered clothes because my children 's collection library . do n't you have credit ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요</td>\n",
       "      <td>i 'm afraid it 's too far behind in the bank . you need cash payment in cash advance . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?</td>\n",
       "      <td>we may have lost the baggage , so we 'd like to make a lost baggage report . would you come with me to the office ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .</td>\n",
       "      <td>busan governor korona , ga and i will be promoted to daegu jija between electric 2000 . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>변기 가 막히 었 습니다 .</td>\n",
       "      <td>the toilet is stopped up . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?</td>\n",
       "      <td>show me that pants are looking at all . how much can i buy this one ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .</td>\n",
       "      <td>i 'd like to go to the duta and take care of the department store . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다</td>\n",
       "      <td>i ca n't wash my hands when i feel bad . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .</td>\n",
       "      <td>the president told me to build a group 's rago . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .</td>\n",
       "      <td>give me some time to look for this . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .</td>\n",
       "      <td>this gaemigundan eun the handle of eul . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .</td>\n",
       "      <td>they useung the grass reul , which changes to naseongbeom the other jinchul iraneun tto dareun kkum 's recent miles . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .</td>\n",
       "      <td>please help us to make a hyogwa to investigate it because we intend to fill in this event . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다</td>\n",
       "      <td>the record shows that video program is geol . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Korean                                                                                                                      English\n",
       "0                                  모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .                             all liquids , gels and aerosols must be placed in a single , zip-top , clear plastic bag . <eos>\n",
       "1                 미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?         excuse me . my child is laundered clothes because my children 's collection library . do n't you have credit ? <eos>\n",
       "2                                            은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요                                 i 'm afraid it 's too far behind in the bank . you need cash payment in cash advance . <eos>\n",
       "3                         아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?    we may have lost the baggage , so we 'd like to make a lost baggage report . would you come with me to the office ? <eos>\n",
       "4                            부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .                                busan governor korona , ga and i will be promoted to daegu jija between electric 2000 . <eos>\n",
       "5                                                                           변기 가 막히 었 습니다 .                                                                                             the toilet is stopped up . <eos>\n",
       "6                                         그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?                                                  show me that pants are looking at all . how much can i buy this one ? <eos>\n",
       "7                                              비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .                                                    i 'd like to go to the duta and take care of the department store . <eos>\n",
       "8                                                    속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다                                                                               i ca n't wash my hands when i feel bad . <eos>\n",
       "9                                                      문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .                                                                       the president told me to build a group 's rago . <eos>\n",
       "10                                                        이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .                                                                                   give me some time to look for this . <eos>\n",
       "11                                                       이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .                                                                               this gaemigundan eun the handle of eul . <eos>\n",
       "12                     통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .  they useung the grass reul , which changes to naseongbeom the other jinchul iraneun tto dareun kkum 's recent miles . <eos>\n",
       "13  이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .                            please help us to make a hyogwa to investigate it because we intend to fill in this event . <eos>\n",
       "14                                                             요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다                                                                           the record shows that video program is geol . <eos>"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Korean as reverse order.\n",
    "- It has no difference.\n",
    "    \n",
    "**Compare Results**\n",
    "\n",
    "|Model|Best Val. Loss|Val. PPL |Test Loss|Test PPL|BLEU Score|\n",
    "|------------------------|----|----|----|----|----|\n",
    "|Forward|0.87|2.40|0.88|2.42|55.86|\t\n",
    "|Backward|0.88|2.41|0.88|2.41|55.72|\t\n",
    "\n",
    "\n",
    "|Type|Sentence|\n",
    "|------|----------------------------------------------------------------------------|\n",
    "|-|그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?|\n",
    "|Forward|show me that pants are looking at all . how much can i buy this one ?|\n",
    "|Backward|how much can i buy ? i see these one .|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZgb-pIMc7gZ",
    "outputId": "35a24c72-08b2-4b9e-b573-c0ee804b17bf",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .</td>\n",
       "      <td>i need a single bag , please . all liquids , gels and aerosols in their carry-on bag . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?</td>\n",
       "      <td>could you change my ticket to gwanghwamun ? my child 's scared to me . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요</td>\n",
       "      <td>you need to get paid . it 's too much of a bank to get back . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?</td>\n",
       "      <td>shall we go along with you ? we have lost your baggage . we must make a missing suitcase form . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .</td>\n",
       "      <td>&lt;unk&gt; &lt;unk&gt; in daegu , &lt;unk&gt; are really passing through at busan . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>변기 가 막히 었 습니다 .</td>\n",
       "      <td>the toilet is clogged up . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?</td>\n",
       "      <td>how much can i buy ? i see these one . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .</td>\n",
       "      <td>that 's good . if you go to duta , you can take it to the department store . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다</td>\n",
       "      <td>it 's good to be polite with breakfast in the morning . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .</td>\n",
       "      <td>i told you all doors in the same group . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .</td>\n",
       "      <td>give me the time to see it for a few weeks . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .</td>\n",
       "      <td>i was okay . all the drain in the body shape is no . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .</td>\n",
       "      <td>hey , turn over and &lt;unk&gt; . the power body was set and set up again in the strap of the new dreams . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .</td>\n",
       "      <td>i think you will force it to develop this restructuring it and it possible to develop and weight , and weight for the company is making of our ability to develop . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다</td>\n",
       "      <td>i wear it &lt;unk&gt; these days . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Korean  \\\n",
       "0                                  모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .   \n",
       "1                 미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?   \n",
       "2                                            은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요   \n",
       "3                         아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?   \n",
       "4                            부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .   \n",
       "5                                                                           변기 가 막히 었 습니다 .   \n",
       "6                                         그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?   \n",
       "7                                              비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .   \n",
       "8                                                    속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다   \n",
       "9                                                      문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .   \n",
       "10                                                        이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .   \n",
       "11                                                       이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .   \n",
       "12                     통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .   \n",
       "13  이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .   \n",
       "14                                                             요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다    \n",
       "\n",
       "                                                                                                                                                                      English  \n",
       "0                                                                                i need a single bag , please . all liquids , gels and aerosols in their carry-on bag . <eos>  \n",
       "1                                                                                                could you change my ticket to gwanghwamun ? my child 's scared to me . <eos>  \n",
       "2                                                                                                         you need to get paid . it 's too much of a bank to get back . <eos>  \n",
       "3                                                                       shall we go along with you ? we have lost your baggage . we must make a missing suitcase form . <eos>  \n",
       "4                                                                                                    <unk> <unk> in daegu , <unk> are really passing through at busan . <eos>  \n",
       "5                                                                                                                                            the toilet is clogged up . <eos>  \n",
       "6                                                                                                                                how much can i buy ? i see these one . <eos>  \n",
       "7                                                                                          that 's good . if you go to duta , you can take it to the department store . <eos>  \n",
       "8                                                                                                               it 's good to be polite with breakfast in the morning . <eos>  \n",
       "9                                                                                                                              i told you all doors in the same group . <eos>  \n",
       "10                                                                                                                         give me the time to see it for a few weeks . <eos>  \n",
       "11                                                                                                                 i was okay . all the drain in the body shape is no . <eos>  \n",
       "12                                                                 hey , turn over and <unk> . the power body was set and set up again in the strap of the new dreams . <eos>  \n",
       "13  i think you will force it to develop this restructuring it and it possible to develop and weight , and weight for the company is making of our ability to develop . <eos>  \n",
       "14                                                                                                                                         i wear it <unk> these days . <eos>  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree\n",
    "\n",
    "enen_list_raw = [] # parse file\n",
    "enen_list = []\n",
    "\n",
    "i = 0\n",
    "with open('ko-en.en.parse.syn', 'r') as f:\n",
    "    for j in f.readlines():\n",
    "        temp = ' '.join(Tree.fromstring(j).leaves())\n",
    "        enen_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enko_list = []\n",
    "with open('ko-en.ko.parse', 'r') as f:\n",
    "\n",
    "    line = []\n",
    "    for j in f.readlines():\n",
    "        if ('<id' in j) or ('<sent' in j) or ('</sent' in j):\n",
    "            continue\n",
    "        if '</id>' in j:\n",
    "            continue\n",
    "        if j == '\\n':\n",
    "            line = ' '.join(line)\n",
    "            enko_list.append(line)\n",
    "            line = []\n",
    "            continue        \n",
    "        \n",
    "        split_tab=j.split('\\t')\n",
    "        text=split_tab[3].split('/')    # get text\n",
    "        line.append(text[0])            # save\n",
    "\n",
    "        for temp_text in text[1:]:\n",
    "            if len(temp_text)==0:\n",
    "                continue\n",
    "            if '\\n' in temp_text:\n",
    "                break;\n",
    "                \n",
    "            text_2 = temp_text.split('|')\n",
    "            line.append(text_2[1])         # save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 .</td>\n",
       "      <td>Flight 007 will stay on the ground for one hour .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 .</td>\n",
       "      <td>Flight 017 will stay on the ground for three h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 , 000 달러 여행자 수표 가 필요 하 ㅂ니다 .</td>\n",
       "      <td>I need 1,000 dollars in traveler 's checks .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 , 250 원 이 공식 환율 이 ㅂ니다 .</td>\n",
       "      <td>The official exchange rate is around 1,250 Won .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100 달러 3 장 과 나머지 는 20 달러 권 으로 주 시 ㅂ시오 .</td>\n",
       "      <td>Please give me three hundred dollar bills and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330969</th>\n",
       "      <td>저 의 주소 가 지방 으로 되 어 있 는데 , 서울 에서 여권 을 만들 ㄹ 수 있 ...</td>\n",
       "      <td>Can I file my passport application in Seoul ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330970</th>\n",
       "      <td>학교 친구 한 명 이 LA 에 있 어요 .</td>\n",
       "      <td>I have a school friend in Los Angeles .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330971</th>\n",
       "      <td>이것 ㄴ 얼마 이 에요 ?</td>\n",
       "      <td>How much is this ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330972</th>\n",
       "      <td>저것 ㄴ 얼마 이 에요 ?</td>\n",
       "      <td>How much is that ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330973</th>\n",
       "      <td>저것 은 얼마 이 에요 ?</td>\n",
       "      <td>How much is that ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330974 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Korean  \\\n",
       "0                     777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 .   \n",
       "1                        777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 .   \n",
       "2                          1 , 000 달러 여행자 수표 가 필요 하 ㅂ니다 .   \n",
       "3                               1 , 250 원 이 공식 환율 이 ㅂ니다 .   \n",
       "4                 100 달러 3 장 과 나머지 는 20 달러 권 으로 주 시 ㅂ시오 .   \n",
       "...                                                   ...   \n",
       "330969  저 의 주소 가 지방 으로 되 어 있 는데 , 서울 에서 여권 을 만들 ㄹ 수 있 ...   \n",
       "330970                            학교 친구 한 명 이 LA 에 있 어요 .   \n",
       "330971                                     이것 ㄴ 얼마 이 에요 ?   \n",
       "330972                                     저것 ㄴ 얼마 이 에요 ?   \n",
       "330973                                     저것 은 얼마 이 에요 ?   \n",
       "\n",
       "                                                  English  \n",
       "0       Flight 007 will stay on the ground for one hour .  \n",
       "1       Flight 017 will stay on the ground for three h...  \n",
       "2            I need 1,000 dollars in traveler 's checks .  \n",
       "3        The official exchange rate is around 1,250 Won .  \n",
       "4       Please give me three hundred dollar bills and ...  \n",
       "...                                                   ...  \n",
       "330969  Can I file my passport application in Seoul ev...  \n",
       "330970            I have a school friend in Los Angeles .  \n",
       "330971                                 How much is this ?  \n",
       "330972                                 How much is that ?  \n",
       "330973                                 How much is that ?  \n",
       "\n",
       "[330974 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = {'Korean' : enko_list, 'English': enen_list}\n",
    "df = pd.DataFrame(raw_data, columns=[\"Korean\", \"English\"])\n",
    "\n",
    "df.to_csv(\"data.csv\",index=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input format using TorchText Field.\n",
    "def tokenize_ko(text):\n",
    "    return [tok for tok in text.split()]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok for tok in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_token and eos_token append <sos>, <eos> automatically.\n",
    "SRC = Field(tokenize = tokenize_ko,\n",
    "           init_token = '<sos>',\n",
    "           eos_token = '<eos>',\n",
    "           )\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en,\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = {'Korean': ('src',SRC), 'English': ('trg',TRG)}\n",
    "data_set = TabularDataset('data.csv',\n",
    "                          format='csv',fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data_set.split(split_ratio=0.8,random_state=random.seed(SEED))\n",
    "test_data, valid_data = test_data.split(split_ratio=0.5,random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': ['전화번호부에서', '찾아보시어요', '.'],\n",
       " 'trg': ['look', 'it', 'up', 'in', 'the', 'yellow', 'pages', '.']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ratio train : valid : test = 8 : 1 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Total examples: 330974\n",
      "Number of training examples: 265109\n",
      "Number of validation examples: 32767\n",
      "Number of testing examples: 33098\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Total examples: {len(train_data.examples)+len(valid_data.examples)+len(test_data.examples)}\")\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab. Use only words that appear more than once.\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. (Packed) Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Iterator.\n",
    "- Using BucketIterator minimizes padding by making the sentence length of the input/output array as close as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make iterator\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     sort_within_batch = True,\n",
    "     sort_key = lambda x : len(x.src),\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                                 \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "                \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        input = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(55932, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(14343, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc_out): Linear(in_features=1792, out_features=14343, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 50,140,679 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, src_len = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, src_len, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src, src_len = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, src_len, trg, 0) #turn off teacher forcing\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 4m 50s\n",
      "\tTrain Loss: 3.489 | Train PPL:  32.768\n",
      "\t Val. Loss: 3.153 |  Val. PPL:  23.418\n",
      "Epoch: 02 | Time: 4m 49s\n",
      "\tTrain Loss: 2.279 | Train PPL:   9.771\n",
      "\t Val. Loss: 2.726 |  Val. PPL:  15.271\n",
      "Epoch: 03 | Time: 4m 48s\n",
      "\tTrain Loss: 1.841 | Train PPL:   6.302\n",
      "\t Val. Loss: 2.558 |  Val. PPL:  12.907\n",
      "Epoch: 04 | Time: 4m 48s\n",
      "\tTrain Loss: 1.594 | Train PPL:   4.923\n",
      "\t Val. Loss: 2.473 |  Val. PPL:  11.858\n",
      "Epoch: 05 | Time: 4m 48s\n",
      "\tTrain Loss: 1.423 | Train PPL:   4.151\n",
      "\t Val. Loss: 2.381 |  Val. PPL:  10.815\n",
      "Epoch: 06 | Time: 5m 16s\n",
      "\tTrain Loss: 1.315 | Train PPL:   3.725\n",
      "\t Val. Loss: 2.361 |  Val. PPL:  10.602\n",
      "Epoch: 07 | Time: 4m 49s\n",
      "\tTrain Loss: 1.229 | Train PPL:   3.419\n",
      "\t Val. Loss: 2.310 |  Val. PPL:  10.071\n",
      "Epoch: 08 | Time: 5m 13s\n",
      "\tTrain Loss: 1.161 | Train PPL:   3.194\n",
      "\t Val. Loss: 2.313 |  Val. PPL:  10.102\n",
      "Epoch: 09 | Time: 5m 18s\n",
      "\tTrain Loss: 1.118 | Train PPL:   3.060\n",
      "\t Val. Loss: 2.259 |  Val. PPL:   9.573\n",
      "Epoch: 10 | Time: 4m 47s\n",
      "\tTrain Loss: 1.072 | Train PPL:   2.922\n",
      "\t Val. Loss: 2.265 |  Val. PPL:   9.634\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.262 | Test PPL:   9.600 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        #nlp = spacy.load('de')\n",
    "        tokens = [token for token in sentence.split()]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "\n",
    "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
    "\n",
    "    mask = model.create_mask(src_tensor)\n",
    "        \n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "\n",
    "        attentions[i] = attention\n",
    "            \n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                       rotation=45)\n",
    "    ax.set_yticklabels(['']+translation)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 46.75\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_list = [\n",
    "'모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .',\n",
    "'미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?',\n",
    "'은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요',\n",
    "'아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?',\n",
    "'부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .',\n",
    "'변기 가 막히 었 습니다 .',\n",
    "'그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?',\n",
    "'비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .',\n",
    "'속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다',\n",
    "'문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .',\n",
    "'이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .',\n",
    "'이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .',\n",
    "'통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .',\n",
    "'이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .',\n",
    "'요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다 ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .</td>\n",
       "      <td>all liquids , gels aerosols aerosols , gels , aerosols in the , , and aerosols in the upper container . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?</td>\n",
       "      <td>i 'm sorry , &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; . &lt;unk&gt; . &lt;unk&gt; . &lt;unk&gt; . &lt;unk&gt; . wo n't you take a &lt;unk&gt; &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요</td>\n",
       "      <td>it 's why not a a big reason . wo n't you have to pay it &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?</td>\n",
       "      <td>i have n't leak out the &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; . &lt;unk&gt; &lt;unk&gt; . &lt;unk&gt; . &lt;unk&gt; . . . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .</td>\n",
       "      <td>the &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>변기 가 막히 었 습니다 .</td>\n",
       "      <td>the &lt;unk&gt; &lt;unk&gt; the the city i was n't . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?</td>\n",
       "      <td>that size for the invoice for me . i think you can . how you you for this for the &lt;unk&gt; for the &lt;unk&gt; ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .</td>\n",
       "      <td>it was the to the the &lt;unk&gt; journey was &lt;unk&gt; and &lt;unk&gt; journey was &lt;unk&gt; and &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다</td>\n",
       "      <td>it is &lt;unk&gt; from the &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; after &lt;unk&gt; and &lt;unk&gt; after the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .</td>\n",
       "      <td>the athletic always is the the &lt;unk&gt; and &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .</td>\n",
       "      <td>this &lt;unk&gt; &lt;unk&gt; this &lt;unk&gt; &lt;unk&gt; , and to extend the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .</td>\n",
       "      <td>cif varies of &lt;unk&gt; &lt;unk&gt; , the &lt;unk&gt; are both acceptable to &lt;unk&gt; . . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .</td>\n",
       "      <td>the author &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; , &lt;unk&gt; &lt;unk&gt; functions . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .</td>\n",
       "      <td>this laws headquarters headquarters headquarters &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다</td>\n",
       "      <td>have this been been performance many &lt;unk&gt; many &lt;unk&gt; performance or &lt;unk&gt; the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Korean  \\\n",
       "0                                  모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .   \n",
       "1                 미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?   \n",
       "2                                            은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요   \n",
       "3                         아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?   \n",
       "4                            부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .   \n",
       "5                                                                           변기 가 막히 었 습니다 .   \n",
       "6                                         그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?   \n",
       "7                                              비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .   \n",
       "8                                                    속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다   \n",
       "9                                                      문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .   \n",
       "10                                                        이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .   \n",
       "11                                                       이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .   \n",
       "12                     통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .   \n",
       "13  이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .   \n",
       "14                                                             요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다    \n",
       "\n",
       "                                                                                                                                             English  \n",
       "0                                      all liquids , gels aerosols aerosols , gels , aerosols in the , , and aerosols in the upper container . <eos>  \n",
       "1   i 'm sorry , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <unk> . <unk> . <unk> . <unk> . wo n't you take a <unk> <unk> . <eos>  \n",
       "2                                                                             it 's why not a a big reason . wo n't you have to pay it <unk> . <eos>  \n",
       "3                                                                i have n't leak out the <unk> <unk> <unk> . <unk> <unk> . <unk> . <unk> . . . <eos>  \n",
       "4                                  the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <eos>  \n",
       "5                                                                                                     the <unk> <unk> the the city i was n't . <eos>  \n",
       "6                                      that size for the invoice for me . i think you can . how you you for this for the <unk> for the <unk> ? <eos>  \n",
       "7                                                        it was the to the the <unk> journey was <unk> and <unk> journey was <unk> and <unk> . <eos>  \n",
       "8                                                         it is <unk> from the <unk> <unk> <unk> <unk> after <unk> and <unk> after the <unk> . <eos>  \n",
       "9                                                                             the athletic always is the the <unk> and <unk> <unk> and <unk> . <eos>  \n",
       "10                                                                               this <unk> <unk> this <unk> <unk> , and to extend the <unk> . <eos>  \n",
       "11                                                                      cif varies of <unk> <unk> , the <unk> are both acceptable to <unk> . . <eos>  \n",
       "12                                                  the author <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> <unk> functions . <eos>  \n",
       "13            this laws headquarters headquarters headquarters <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <eos>  \n",
       "14                                                      have this been been performance many <unk> many <unk> performance or <unk> the <unk> . <eos>  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in sen_list:\n",
    "  translation, attention = translate_sentence(s, SRC, TRG, model, device)\n",
    "  pred = ' '.join(translation)\n",
    "  final = {'Korean': s, 'English': pred}\n",
    "  df=df.append(final, ignore_index=True)\n",
    "\n",
    "\n",
    "pd.set_option('max_colwidth', 200)\n",
    "df = df[['Korean','English']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLPi2gDh-JpJ"
   },
   "source": [
    "# 3. Convolutional Sequence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfl3WI06-JpJ",
    "outputId": "9eebfd8e-3b9f-46b1-dd3e-6837df51384b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     sort_within_batch = True,\n",
    "     sort_key = lambda x : len(x.src),\n",
    "     device = device)\n",
    "\n",
    "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "#     (train_data, valid_data, test_data), \n",
    "#      batch_size = BATCH_SIZE,\n",
    "#      device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4YS2dEBM-JpJ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size, \n",
    "                                              padding = (kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            conved = conv(self.dropout(conv_input))\n",
    "\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "\n",
    "            conv_input = conved\n",
    "        \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        combined = (conved + embedded) * self.scale\n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rxnE7PDB-JpK"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 trg_pad_idx, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        \n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        combined = (conved_emb + embedded) * self.scale\n",
    "        \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        \n",
    "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
    "        \n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        \n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "        \n",
    "        return attention, attended_combined\n",
    "        \n",
    "    def forward(self, trg, encoder_conved, encoder_combined):\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "            \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        batch_size = conv_input.shape[0]\n",
    "        hid_dim = conv_input.shape[1]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            conv_input = self.dropout(conv_input)\n",
    "        \n",
    "            padding = torch.zeros(batch_size, \n",
    "                                  hid_dim, \n",
    "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
    "                \n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
    "        \n",
    "            conved = conv(padded_conv_input)\n",
    "\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            attention, conved = self.calculate_attention(embedded, \n",
    "                                                         conved, \n",
    "                                                         encoder_conved, \n",
    "                                                         encoder_combined)\n",
    "            \n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            \n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "         \n",
    "        output = self.fc_out(self.dropout(conved))\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pI1P_Zjh-JpL"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        encoder_conved, encoder_combined = self.encoder(src)\n",
    "            \n",
    "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HcuiMFq-JpM"
   },
   "source": [
    "#### Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "kxFfrngJ-JpM"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
    "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
    "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
    "ENC_KERNEL_SIZE = 3 # must be odd!\n",
    "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "    \n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpduuOOg-JpM",
    "outputId": "f02faac2-6df5-4af7-f14a-c7106898df29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 43,838,471 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_I269V74-JpN"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qRuXye2y-JpN"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "eLTq9jIL-JpO"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ODVi6MNY-JpO"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zE6O8Lsq-JpQ"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xH5_9a-c-JpR",
    "outputId": "b93a050b-5f94-409d-e5df-b2514b9559ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 5m 42s\n",
      "\tTrain Loss: 3.242 | Train PPL:  25.583\n",
      "\t Val. Loss: 2.096 |  Val. PPL:   8.136\n",
      "Epoch: 02 | Time: 5m 46s\n",
      "\tTrain Loss: 2.248 | Train PPL:   9.469\n",
      "\t Val. Loss: 1.763 |  Val. PPL:   5.830\n",
      "Epoch: 03 | Time: 5m 47s\n",
      "\tTrain Loss: 1.979 | Train PPL:   7.238\n",
      "\t Val. Loss: 1.613 |  Val. PPL:   5.018\n",
      "Epoch: 04 | Time: 5m 46s\n",
      "\tTrain Loss: 1.805 | Train PPL:   6.082\n",
      "\t Val. Loss: 1.500 |  Val. PPL:   4.480\n",
      "Epoch: 05 | Time: 5m 46s\n",
      "\tTrain Loss: 1.937 | Train PPL:   6.939\n",
      "\t Val. Loss: 1.603 |  Val. PPL:   4.968\n",
      "Epoch: 06 | Time: 5m 46s\n",
      "\tTrain Loss: 2.613 | Train PPL:  13.644\n",
      "\t Val. Loss: 2.275 |  Val. PPL:   9.730\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 6\n",
    "CLIP = 0.1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Sj2ytY4-JpS",
    "outputId": "c25b37f0-4e01-4ea0-852f-d802951f4537"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.314 | Test PPL:  10.120 |\n"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load('tut5-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgERx28d-JpT"
   },
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "wxcuIMWf-JpT"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        # nlp = spacy.load('de')\n",
    "        # tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "        tokens = [token for token in sentence.split()]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_conved, encoder_combined = model.encoder(src_tensor)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "xT_Xt3TP-JpT"
   },
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "        \n",
    "    attention = attention.squeeze(0).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                       rotation=45)\n",
    "    ax.set_yticklabels(['']+translation)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAZC8FM5-JpV",
    "outputId": "9487f726-9d51-40b6-f566-5ea1cb7b68d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['?', '나요', '가', '게', '어떻', '에', '유적지', '암사동']\n",
      "trg = ['how', 'can', 'i', 'get', 'to', 'amsadongyujeokji', '?']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 2\n",
    "\n",
    "src = vars(valid_data.examples[example_idx])['src']\n",
    "trg = vars(valid_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5BQNN0e-JpV",
    "outputId": "b6e9df77-7886-4560-dd7b-06b729000fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['how', 'can', 'i', 'get', 'to', 'amsadongyujeokji', '?', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wGENNXTG-JpW",
    "outputId": "61cc5979-7779-42a5-e02c-7c63f70cc232",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45208 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50836 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44032 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44172 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50612 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46523 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50976 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51201 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51648 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50516 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49324 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46041 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45208 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50836 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44032 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44172 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50612 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46523 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50976 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51201 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51648 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50516 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49324 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46041 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAIGCAYAAABAotnIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debhkVX3v//enaaUZBOGKiCIS5/FGpNEoKDjhgIojXiURCIIBcUg0cchVcc4PY3DgqqAiYIyiIsYxOAAKRgzdiuKACk6AgAgIMnQzfX9/7H2gLE8vuuk+p4bzfj1PPXX2rl21v+tU1anPWbX22qkqJEmSJM1u0agLkCRJksaZgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZKkiZckc/XYBmYtWMNvrCSLZlsvSZLGW5JFVVX9z5sn2S/Jnuvs8fvHlhaUJBl4Y20G7AbcHvhgVa0caXGSJGm1zHyeJ7kNsDHwJmBL4NnASuBewPm1loF38VpXKk2QgaB82ySbcvMba3fgHODL/bUkSRpzfVjeBXgu8CzgN8AZwJXAu6vqvHWxH4dkaEHp31iPBd4F/AjYDvgdcBXwiaoyLEuSNAGSHJDkKOBrwN2Aw4CHAicAPwC+2m+31kMt7WHWgpHkQOCRdP+FfpnuP8+3JHkq8CDgG/12WduvbiRJ0txIsjHwcuAVwOl0wy9Orqo/9Lfv2296CnSdZWu7TwOzpl4/runfgKcCZwFPAb5VVZf3mxwAUFVf668Ny5IkjamqujLJZ4DjgAur6rKZXuQkz6DrBHt+/63yoqq6cW33aWDW1Kuq65K8h+6rmt/1b6yZGTGeBjwA+Nt+eb2qumF01UqSpFVJcpeqOr+qfjywLnTDjG8AHgWcD5wNsC7CMhiYNeWSbAVcUlU/H1gXYGY806OAS4GfARiWJUkaT0neAWyb5F1V9a2Z9f03wzckeSDdt8YHVdX563LfHvSnqZXk3cBbgB0H11fnhiQPAl4MvL+qzh1FjZIk6ZYl+RTdLBgnAr+d5fbbAs+jG3r5hXW9f3uYNZWSfBLYnm4Yxk9nuX09uqnkzqI/ilaSJI2fJK8HHkx30P6ZVbWiPz5p0cC5E24ENgNOr6oL13UNBmZNnSSvBf6K7qjZH/RvrPWramWS21bVtcASuoMCTq2qX46yXkmS1HR34JSqOh0gyf2A1wJ3TrIceFN/IOD7gZ/026zTGa8MzJoqfc/xvYDPV9X/9OvuC7w5ye2AS5L8Q1Vd1AfrC/ttnEpOkqQx0n+mrwdsDlyfZDfgfwOvA84EfgW8BLgW+L9VdebMfdf1Z7qnxtbUSfIhuvmW/w7YGXg1cBpwGV2YPhV4ad/TLElaQJLcBbh4Gj8DprVtSR5MNy65gD8AH62qQ/qD+D8GbAo8dV3NiDEbe5g1jf4VuAfdm+tHwOuq6h39G+t4YMtp+2MiSbplSV4F7Aq8Lcm3q+rqUde0rkxT25I8G9iGruf4pKo6I8n2wO3oOntnZr7aHNiCbhjGnPYAG5g18ZI8F9gauAZYXlXfAR6d5H8Dlw3MgLEJ3RvqoiSLgRschiFJC0OSfwTuQ3cw+G79uokOljOmqW39QfsPB27Tr3pnklcAR1bVRQPb3Qd4Fd0QjYPm+vPcwKyJ1k8zszNwNXBnujB8XFW9vKp+MLDddnRzMz4S2Kmqrh9JwZKkedcHygdX1Z798oZ0MyVNbLCcMU1tS3Iw3VSwLwDOALYC9gPeRZdZ39V/W/wvdL3pGwO7VtWfzYa1rhmYNbGSvA54GN1sGKfTfS3z98Ce/WwYB/bbHQDsRTfdzGOr6qwRlSxJmmd9oPzLmUAJUFUfS3IDExosZ0xT2/p5lJcCX6+qr/erL0nycrpvkN+R5NSqWtb3Ql8DHD1fM10ZmDXJtgO+C3y7qq4DfpPk7XS9zXsl+QbwSeDXwL8DX6iqX42qWEnS/EqyL/D0qtqxX74NcH1/AqtPdJ2VkxUsZ0xh266jG4axySy3vRd4CrBvku9X1fIk35vLg/yGeaY/TZwki/r/RLcCrqmq65IsTrKoqn4HvI/ufPKP6P9wfAl4n2F5/PVjy6Wx42tz8iRZAlwFnJHkrwCq6rqqqv5rfarqE8C36ILlI/rhDGNv2to2MLXr2cAO/SmugZvOzns+8EfgDn0HGfMZlsEeZk2YJBsDV1XVtUmOA96aZMeq+lYfmquqzk/yQ+BeSdarqhvm+42lNZfkRcANSf6zqi4edT3SDF+bk6d/zq4Fvt1f75Nkg6o6CboQNhPSqurYPmM+rb/vf49zb+w0tS3JVnRn6LsGuAL4J+AJwPuT7FVVv+i3uxNdW3+VZBF9jp7PWg3MmhhJDul/PAr4MfB54HHA+5Ls38+OQZItgP8FnFZVN4yiVq2Zfhzek4EfACuSnGAw0TjwtTl5+udsN7qDxq7urwGem4TVDJbfrqqrRlB+0zS1LckH6YZW3h1YnuTYqvpQkucDnwD+K8nH6c6hsCPdbBj7j6oDzMCsidAP8H8IcAzdm4eq+mmSw4GXASf0gXoR8ADgvsDeo6lWa2LgoJVHJ7k/cFC/3mCikfK1OXkGnrNdkjwAOBAIqx8sQzf7QpKcOE6dLtPUtiTHAI8G3gIsAe4KHJFky6p6a5KHAR8CnkM3E8YvgJ1rHmbDWGXN89yjLa2xJG8D9gT2AH5YVVclWb+qVva335vuD8fzgCuB3wAvq4Fp5TSeMjQdUr9uKbAP3deNBhONhK/NybOK52wHulmSTqULlvenC43HzgTLfrv1ZgJkkiPpDjz766paMY9NWKVpalu6E5B8lO4svF/uj0N6KN0ZeY8EXjzw+X5HulNjX1lVfxxFvTPsYdZYS7IJ8CDgwwNDLv4CeFV/24+Ad1XVy/tgfRXdP4JXjqxorZYkrwHuXTfPHbq4qq6vbsog6IKJvXmad742J0/jOTu9f8726jf9s97Yvuf1xv5+ewLbA88fo7A8bW27Hd1Z/C7uw/I9gf8CPg68tKpWJtmuqr5X3YH8Y8HArLGUZOOqurKqrugH+P9VkocAu9B9hXMm3X+dS4Grk7wbuGScvj7TqvUHcPwl3R/JGTc9d9MUTJJsCfxuvg9QmS/T1j5fm5NnNZ6zVrBcVP2cv0n2Al4JPK+qfjTnha+GaWnbzGd6vziTPX/bH3P0P8BXgRdV1dVJngnskeRlNXBmv1FzWjmNqy8leWf/8wfpTjpyKvC3wNuq6mF0p868ErhfVd1oWJ4MfW/Je4H3A/dMciDcPN5uZruqWgZ8hO55fkL/h3WiJHkV3bj7R/b/+E2VaWufr83JswbP2enA0cBOwIPpDhz/CvCcJA/oQ9pMoPzhPDdjVlPWtps+06vqa8D3ga8DPwM+B7ywqq7s/4l7FlB0BzWODXuYNXb6cVn/i264BcAX6E5QsjVwWVX9pP9jsTHwO+DimT8ek95TMiPdPNP3Aa4HfjMORzSvC/04vHsDnwEeS/dH/UlJXlRVhw8epAKT3ZvXt/W+wIe5+eQBp47qCO91bdra52tz8p67W/GczdYbW8ChwD2A3ccoLE9N2wY+0384UPO/AG8A7gi8sar+mO54pNcAjwEePeoxy8MMzBpHT6DrUT4FoKqupzuQ7zcD29wPeDndzBkvmZagDJDkdnT/JNyDbtjJ6Un2nZQP41XJ0EEr6c5S9QS6rxqfeAvBZBHdB0El+XJVXTqqdqyOWdq6IfDM/ueJCybDpq19vjYn77lbi+fs9P45+xu6sb3L6ULlz6rqnJE0ZsgUtm3mM/3Ugc/qE4CN6A78W57kV3QBf3PgSVV11igKbXGWDI2VJPcFTgL+taremVkmKE/y98BTgb+gOy3o90dT7bqX7uxNX6c7ePFwul7mfYFjquqNo6xtbeTm6ZD+emj93sC96HpPngj8qqoO729L/4GwqKpuTHJXuiOoD6+qT89vC1Zfo61/Q3fAzWfoPjgmIpgMm7b2+dqcvOduHT1n29D1sL8fOH5cOl2mrW2r+ExPVd3QfzO8Ad1Qy42AXwPfqqpzR1Vvy8SOW9J0SbJe/+NSunmW/3vmtv4Pwabp5p2kv/1k4HHTFJZ7j+mvX1lVx1XV2+jGeO0wwprWSpL9gKfMfACkOyPjzBCao4Cf002F9F/AtunOYjXzvC+m63WA7newLTcf1DJ2bqGtH6Xr8XkmsFMmcNzotLXP1+bkPXfr8DlbStfp8oMxCstT07Zb+Ey/IclmwP2r6uqqOqyq/r+q+sS4hmUwMGtM9G+gRcDrgO9V1bf7mzZK8mS6s/6cmeQldAc3vH1cvj5bx86l+0/75zN/KOk+yNYHGOcPstn09V5HN3btwdANsZn5KrFfPopVfxDMbLsX8EbgGVV19giacotWs60TE0yGTVv7fG1O3nM3B8/Z08flOZu2tq3GZ/p/AN9Pd2Aj0PWUj6DU1TZ2bwgtPANvkr3p/hP91379a4Fj6U6B/QfgAOAD1bluBKXOuao6E9ixqq4BZn4vK4C7J9lk3L8qHdbX+0ngW8CBSR4xcNtqfRCkO03qKxijo9dnswZtHftgMptpa5+vzZtum5jnbpqfs2lq2xp8pr8YOGTmfuPS078qHvSnkRt4k2wP3B7YO8kRdF9xHgc8vqpOnNk+ufkgh2lU/YwYA+H4XODOdB9gX6nuIMiJUd28mp/tF/dKN6H+f/e3DZ669ah04/RmDm55TLpTot+HbqL9sflwW5U1aOtH+8+UZ8HkHGw1be3ztTl5z900P2fT0rZp/Uz3oD+NhXTjk8/sFz8PnE/3tdKl1Z0JKIN/MEZW6IgkOZluiqHH1BgePbw60h2N/3RgZ+DomQ+C/rabntck+9CdBepk4LnA+0b9AbCm1qCtfwM8EPgS8M1JeW1PW/t8bd5028Q8d9P8nE1D26bxM93ArLHQ/4F4Id3XNJ+vqsv69RPzZpoLufmo582Bvarq0HRHHZ89aT3NcIsfBItn2pTkY3ST1h9UVStHUuxaWoO2Hg3cFth7kto6be3ztXnTbRPz3E3zczbpbZvGz3SHZGgs9F9F/b8aOFvfJL+x1pWZr0Krm9v10HRn53o73SnCvznC0m6VVX3lmG6s5A1w0zi8BwJ7jtMHwJpag7Y+mO5r1Ilq67S1z9fm5D130/ycTXrbpvEz3cCssVFDp7ae5DfWHPow8Ei6r7cm0iwfBIuq6lSAdEd4v4IxGIe3LqxBW3+0ygcZY9PWPl+bk/fcTfNzNultm7bPdAOzNEGq6vdJnlZjdBDOrTH0QfA3SS4EHgS8kjE7en1tTXtbp61909aelmlp67S0YzbT3LZJ4xhmSSPTj3PbnW6s293o5g6dyg+AaW/rtLVv2trTMi1tnZZ2zGaa2zYpDMySRirJBnTTI/2wxuQkAnNl2ts6be2btva0TEtbp6Uds5nmtk0CA7MkSZLUMFZn8ZEkSZLGjYFZkiRJajAwS5IkSQ0GZk2VJPuPuoa5Ms1tA9s36Wzf5JrmtoHtm3Tj0j4Ds6bNWLyx5sg0tw1s36SzfZNrmtsGtm/SjUX7DMySJElSg9PKaU4l8QUmaZ3bfvvt53V/F198MVtsscW87W/58uXzti9Jf+L3VfVnb3YDs+aUgVnSXJj2z64koy5BWqiWV9XS4ZUOyZAkSZIaDMySJElSg4FZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkBgOzJEmS1GBgliRJkhoMzJIkSVKDgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBuYpkuSoJMtGXYckSdI0MTBLkiRJDQZmSZIkqcHAPIWSPD7JD5JcleTUJA8YuG3DJO9JcmGSFUlOT7LrwO379Pe7zcC63ya5JEn65UVJ/pBkv/ltmSRJ0vwzME+fbYB3AG8FngfcETh2JuwCHwT26W9/BnAu8MUkO/W3nwJsCDwEIMm9+se4HXD/fpu/BDbtt5UkSZpqi0ddgNa5zYEdq+rn0PUGA8cD9+lD8/OAfarq6P72E4AfAK8DnlBVZye5AHgk8J3++vvAtf3PP+qvL66qs+a1ZZIkSSNgD/P0+dVMWO79uL/eGtgBCPCpmRur6sZ+eaeB+5xCF4oBHgV8s78Mrjt1VQUk2T/JMmfskCRJ08DAPH3+MLR8bX+9BNgKuLKqrh7a5iJgwyTr98unADv1PdKP7JcHQ/RONIZjVNURVbW0qpbe+mZIkiSNB4dkLCwXABsn2XAoNG8JXF1VK/vlU+iGdjwe+It++XrgLv0Bglvi+GVJkrRA2MO8sJwOFPDsmRV9L/Kz+dMhFmfS9VT/M3BWVV1cVZcBP+zXXQl8b76KliRJGiV7mBeQqvpJko8DhyW5HXAOsB9wX+CAge1uTPItYDfg8IGHOAV4MfDVqrph/iqXJEkaHXuYF579gKOB1wP/CdwNeEpVDR/ENzPk4puzrFvlAX+SJEnTJlU16ho0xZL4ApO0zk37Z9fNU+dLmmfLZ5u0wB5mSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg4FZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkBgOzJEmS1LB41AVoIcioC5gzS5ZsNOoS5tSKFVeOuoQ5s/76G466hDm1cuXVoy5hTiXT+3cF4GcXXDDqEubUA7bZdtQlzJnrrls56hI0B+xhliRJkhoMzJIkSVKDgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg4FZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkBgOzJEmS1GBgliRJkhoMzFMuyaOSnJTkyiSXJzk5yXZJtkpyZJJfJLkmyc+SvCXJbQfuu22SSrJHksP7+5+X5I1JfO1IkqQFYfGoC9DcSbIL8FXgJGAv4CpgR+AuwPXApcA/AJcB9wYOBrYAXjT0UIcAxwHPBh4LvB74EfDJOW6CJEnSyKWqRl2D5kiSbwO3AXaoW3iikywG9gCOBDapqmuTbAv8EvhoVb1gYNszgLOq6v+sRg0FufWNGHNLlmw06hLm1IoVV466hDmz/vobjrqEObVy5dWjLkFr4WcXXDDqEubUA7bZdtQlzJnrrls56hK0dpZX1dLhlX6tPqWSbAQ8DDh6trCczsuT/DjJNcB1wMeA9YFthjb/ytDyj4GtG/veP8myJMvWqhGSJEljwMA8vTaj69pdVTfFy4F/BY4HdgceCry4v23J0LZ/GFq+dpZtblJVR1TV0tn+Q5MkSZo0jmGeXpcBNwJbreL25wCfrqp/nlmR5P7zUZgkSdIksYd5SlXVVcB3gBckmW0Q8QbA8ECrPee8MEmSpAljD/N0ezXwNeDLSY6gmyXj4cAyutkzXprkO8A5dGH5nqMqVJIkaVzZwzzFquqbwOOBDYF/B44FdgbOA94EfBx4S399LfDS0VQqSZI0vuxhnnJV9Q3gUau4eZ9Z1t00fKOqfsUsc8JV1d7rojZJkqRJYA+zJEmS1GBgliRJkhoMzJIkSVKDgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg4FZkiRJalg86gK0ENSoC5gzK1ZcOeoSdCutXHn1qEvQWqia3r8rAElGXYKkAfYwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkBgOzJEmS1GBgliRJkhoMzJIkSVKDgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMy6VZIclWTZqOuQJEmaa4tHXYAm1puBDUZdhCRJ0lwzMOtWqapzRl2DJEnSfHBIhm4Vh2RIkqSFwsAsSZIkNTgkQ+tckv2B/UddhyRJ0rpgYNY6V1VHAEcAJKkRlyNJkrRWHJIhSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg7Nk6Fapqr1HXYMkSdJ8sIdZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkBgOzJEmS1GBgliRJkhoMzJIkSVKDgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNSwedQGabttvvz3Lli0bdRlzJsmoS5AWJN97kuaTPcySJElSg4FZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkBgOzJEmS1GBgliRJkhoMzJIkSVKDgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDs9ZIkv2TPH3UdUiSJM0XA7PW1P6AgVmSJC0YBmZJkiSpwcC8gCQ5KMm5Sa5K8tkkj01SSXbpb1+U5NVJzk6yMsnPkuw1cP+Tge2Bvfr7VZK9R9IYSZKkebJ41AVofiR5BvBe4H3AfwI7AR8e2uy9wF7Am4DvAo8HjkxySVV9ATgQOA74BfDm/j7nzH31kiRJo2NgXjheC3ypql7cL38lyR2AAwCS3LP/eZ+qOrrf5mtJtgLeAHyhqn6c5Crg4qo6bVU7SrI/3Vhnttlmm7lpjSRJ0jxxSMYCkGQxsB3wuaGbBpcfC9wIHJ9k8cwF+Drw4CTrre7+quqIqlpaVUu32GKLtS1fkiRppOxhXhjuAKwHXDy0/uJZtrl8FY+xFXDeui9NkiRpvBmYF4bfAzcAw929g8uXAtcDO9L1NA/73dyUJkmSNN4MzAtAVV2f5HvA7sDhAzc9beDnE+l6mDetqq82Hu5aYMm6r1KSJGk8GZgXjrcDxyU5jG7s8o7Abv1tN1bVT5N8APhEkkOAZXTB+AHAvavqhf22ZwFPSPIE4BLgl1V1yXw2RJIkaT550N8CUVWfAV5Kd5a+zwI7AK/sb76iv34x3XRxLwC+BBxFF6q/OfBQbwF+AnwSOB146hyXLkmSNFKpqlHXoBFJ8n+BfwY2r6pr5mIfS5curWXLls3FQ4+FJKMuQZIkrTvLq2rp8EqHZCwQSbYAXgOcBFwNPBJ4FfDhuQrLkiRJ08DAvHBcC9yXbrjFpsAFwLuB142yKEmSpHFnYF4gqupy4MmjrkOSJGnSeNCfJEmS1GBgliRJkhoMzJIkSVKDgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpIZU1ahr0BRL4gtMkvQnpjl7JBl1CVo7y6tq6fBKe5glSZKkBgOzJEmS1GBgliRJkhoMzJIkSVKDgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg4FZkiRJajAwS5IkSQ0GZt0kyR5J9h51HZIkSePEwKxBewB7j7oISZKkcWJgliRJkhoMzAIgyVHAs4Cdk1R/Obi/7aAkP0+yMsnZSf5+lLVKkiTNp8WjLkBj483ANsDtgQP7decl2Q94L/BvwAnAo4F3Jlm/qv5lJJVKkiTNo1TVqGvQmEjyaeAOVbVLv7wIOBf4SlXtM7Dd+4A9gS2rasUtPKYvMEnSn5jm7JFk1CVo7SyvqqXDKx2SoZatgTsDnxpafyywCfCg2e6UZP8ky5Ism+P6JEmS5pxDMtSyVX990dD6meXNZ7tTVR0BHAH2MEuSpMlnD7NaLuiv7zi0fsv++tJ5rEWSJGkkDMwadC2wZGD5POC3wHOGttsDuAI4c57qkiRJGhmHZGjQWcDuSZ7OzWH5YODwJJcAXwV2Bg4AXntLB/xJkiRNAwOzBr0P2A44EtgMeGNVHZxkCfCy/nIe8IqqOnR0ZUqSJM0fp5XTnPKgP0nSsGnOHk4rN/GcVk6SJElaUwZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg4FZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkhsWjLkCSJC0sSUZdwpypqlGXMKem+blrsYdZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkBgOzJEmS1GBgliRJkhoMzJIkSVKDgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktQwtYE5yQOTVJJdRl3L2kqybd+Wp8zBY+/dP/bGjW2OSrJsTe4jSZI0LRaPugCtlguAhwNnjWj/bwY2GFj+Il09V4+mHEmSpPljYJ4AVbUSOG2E+z9naPli4OIRlSNJkjSvVmtIRpKHJ/lckguSXJXkjCR7Dtw+8xX9Q5KcnOTqfpuHJNkoyUeSXJ7kF0meN/TYOyU5JckV/eWMJM8ZuP0FSU5NcmmSy5KclGTpLDUemFwte8AAAA8bSURBVOTcvr7PA1vNss2GSd6T5MIkK5KcnmTXoW1OTvLpJM9PcnZf05eTbD203Tb9+muS/LL/HXw6ycn97fefbUhIko2TXJnkZYP7G9pml/6+D+yXZx2SkeSFSX6UZGWSXyf5p1navEeSM/ttzk3y1iTNf5SS/GP/+3lav+yQDEmStGCt7hjmuwHfAvYFngocB3xkOPwCRwMfB54FBPg08GHgt8Czge8Ax8yEzySbAF8AftHf59nAR4HbDzzmtsAxwHOA5wPnAqckufvMBkl2B/5f/1jPBM4EjpylHR8E9gHeCjyjf6wvJtlpaLuHAQcBrwD2Bx4CHDGwvwCfA+4H/C3wD8BL+/sBUFU/pusV3nvosZ8D3Ab491nqW21J/hF4P/BZ4Cn9z29OctDANrsCxwLfBXYH3gu8Ejis8bivB94IPK2qPrc2NUqSJE2FqlqjC10QXgwcDpzYr9sbKGCvge2e3K87cmDdpsB1wAH98tJ+m9ut5r4X9fs+C3j9wPr/Ab48tO0H+8fepV++H3DjUI2LgB8CJwysOxm4HNhsYN3L+8faoF/erV/eYWCbu/RtO3lg3QuBK4GNB9Z9E/j00P4+PVT7Lv3jP7Bf3rZffkq/vEn/uG8Yut+bgAuB9frl04CThrb5J+AGYOuh525j4G3AH4Gdh+5zFLBsYPmm+6zGc1ZevHjx4sXLQrlMu1H/fufhclPeGbys7pCMzfqhDL+mC4XX0fW83nto068P/Hx2f33izIqqupxu7Otd+lXn0AW//0iye5LBnuWZfd8vyfFJLqILetcB95nZdz+84CHAfw7d9TNDyzvQhf1PDdRzY7883MN8elVdNrD84/56pu4dgAur6vSBxzofWD70OMf218/pa71Hv6+PDLdzDT0c2Aj4VJLFMxe63/WWwNZJ1qP7vXxq6L7H0v2j8PCh9f8GHAjsWlXfWJvikuyfZNngMA5JkqRJtbpDMo4Cngu8A9iVLjAeCSwZ2u4PAz9fO8u6mfVLAPpQ+ni6IQqfBC5O8sWZ4RZJbgd8Bbgr3bCHR/b7/v7Avu8ArAf8bmg/w8tbAVdW1fDMDhcBGyZZfxXtGGzLzD7vxOwHvf3Juqr6Y9+uffpVe9P1AP/XLPddE3for3/Ezf/AXAec1K+/a7/NbejaN2hmefOh9c+iC/yns5aq6oiqWlpVfzbWXJIkadLc4iwZSZbQjZF9cVV9YGD9OpnDuapOA56YZAPgcXQ9nf8B/BVdL+jWwOOr6qYp1ZJsOvAQv6freb7j0EMPL18AbJxkw6HQvCVwdXUzUayuC4EtZlm/BbBiaN2HgFOT3At4AXBMVd0wcPsK4LZD99nsFvZ/aX/9FP48EAP8lG7Kt+v489/DlkOPMeMpdGPAj0ny133vuyRJ0oK3OqF3/X67mwJl3/P7tHVZSFVdU1Wfp+u5vn+/embu38F9P4JuTO/M/a4Hvkd3UNugZw4tn043NuXZA4+VfvnUNSz3dOBOSR468Fh3AbYf3rCq/psuwB4JbEPXWz/oPOC+Q+t2pe3bwDXAnatq2SyXP/ahfDn9cJABe9CN5f720PozgSfRBecPIEmSJGA1epir6vIkpwOvT3IFXdh6Nd2BcZuszc6T7EY3y8Rngd/QjRF+ETePez6NbozzB5McQtfbfDBw/tBDvQ34TJL3A8cDOwNPHGrHT5J8HDisD/znAPvRhdUD1rD0L9ENC/lkktfQhdc30PX2ztYz+2G64SzfHuwp7x0P7JvkULoTgjx6uPZhVfWHJAcD705yN7oDCRfRjet+dFU9o9/0DcAJST4CfAJ4EN1JSD5YVefN8rj/k+SpwJeTXFFVr7yF34MkSdLUW91hFc+nm/rtGODddNPKHbMO9n82Xa/v2+jGKh9CN773bwGq6iK6HtI70R3U93Lg77j5gEL67Y4HXkI35d1nge3opsAbth/d1Hev7x/vbnQzT6xRD3N/lOjudLN1fITud/J+uoMDr5jlLp/tr/9sqruq+iLwWrqe7uP7ml62ql0P3O8QugMvn9S35ePAnsApA9t8Bfg/dLORfJ7u9/dOuinzVtW2b9D1zr8kyRtm27ckSdJCki77aW3146p/ARxWVW8Yuu1Aun8G7lxVswXqW3rsBwE/AB5RVcNDKeZckuPoppB7wq24ry8wSdKCMe25qhvNOtWWzzZpgafGvpWS/B3d8Iuf0x3s9w90472PHNhmW7phEq8FjrqVYXk7uh7hq4GfrG3da7jvzYBH0c0L7bhmSZK0IBmYb70VwKvohlAU3clTHldVvx7Y5mC64SzfAF53K/fzYbpZM/arquHp7ubaznRnXjyRbiiHJEnSguOQDM0ph2RIkhaSac9VC3VIxjqZS1mSJEmaVgZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg4FZkiRJajAwS5IkSQ2LR12AJElaWKpq1CXMmSSjLkFzwB5mSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg4FZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkBgOzJEmS1GBgliRJkhoMzJIkSVKDgVmrJclzk3wjyWVJLkhyaJL1R12XJEnSXDMwa3W9EzgFeCbwNuAA4OBRFiRJkjQfUlWjrkETIMkWVXXxwPJHgO2q6sG3cD9fYJKkPzHN2SPJqEvQ2lleVUuHV9rDrNUyGJZ7fwFcNopaJEmS5tPiURegyZPkn4EdgceNuhZJkqS5ZmDWGkmyD/AWYJ+q+sao65EkSZprBmattiS3AQ4FDquqoxrb7Q/sP191SZIkzSUDs9bElsCmwNdaG1XVEcAR4EF/kiRp8nnQn9ZEAT8FLh91IZIkSfPFHmattqo6H7jvqOuQJEmaT/Ywa7UluVuS65M8edS1SJIkzRcDs9ZEgPXwdSNJkhYQh2RotVXVr+hCsyRJ0oJhT6EkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg4FZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkBgOzJEmS1GBgliRJkhoMzJIkSVKDgVmSJElqWDzqAjTd7nDHu/Cs5x006jLmzIcOe92oS5hT6603vX8iNtro9qMuYU5ddtmFoy5hTj384U8fdQlz6sILfzHqEuZUklGXIK0Re5glSZKkBgOzJEmS1GBgliRJkhoMzJIkSVKDgVmSJElqMDBLkiRJDQZmSZIkqcHALEmSJDUYmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg4FZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJjHXJJ7jGCfd0qy4XzvV5IkaRwZmMdQkiVJ9kxyIvDzgfWLkrw6ydlJVib5WZK9Zrn/QUl+3m9zdpK/H7p96ySfTPK7JNckOSfJmwc2eSJwQZLDk+wwZw2VJEmaAItHXYBulmQ7YF9gT2BD4HPAbgObvBfYC3gT8F3g8cCRSS6pqi/0j7Ffv92/AScAjwbemWT9qvqX/nGOATYA9gf+ANwduO/Afo4HNgH2AfZPcibwIeDfq+rSdd1uSZKkcWZgHrEkm9IF5H2BhwBnAG9gKJwmuSdwALBPVR3dr/5akq367b+QZBFwMHBUVb2i3+Yr/T5ek+RdVbUCeCjwvKr6fL/NyYM1VdXlwHuA9yR5CF1wfgNwSJLjgQ8DX6+qWkWb9qcL42x8u9vful+MJEnSmHBIxggleSJwAfBm4FvAdlW1XVW9Z5ae3McCNwLHJ1k8cwG+Djw4yXrA1sCdgU8N3fdYuh7jB/XLZwBvT7J3km1aNVbVd6vqJf3j7gVsRtdz/YvGfY6oqqVVtXTJBhvd0q9BkiRprBmYR2slcDWwBNgUuH2SrGLbOwDrAZcD1w1cjqL7pmCr/gJw0dB9Z5Y376+fCywDDgV+neSMJI+9hVpvqpHudXPZLWwvSZI0FRySMUJVdVKSuwDPAF4InAj8KslRwNFV9euBzS8Frgd2pOtpHvY7bv4H6I5Dt2058BhU1fnA3v0QjofSDeP4XJJtquqSmTv14f0xdEMynglcC/wHcEBVfe/WtFmSJGnS2MM8YlW1sqo+UVWPA+4BfAzYD/hlkq8l+et+0xPpepg3rapls1yuBc4Dfgs8Z2g3ewBXAGcO7fvGqjoNeCPdQYZ3A0iyZZKDgV8CXwPuCvwdsFVVHWhYliRJC4k9zGOkqn4JvK4Pq0+k63X+CN0BgD9N8gHgE0kOoRtSsQR4AHDvqnphVd3Y3/fwJJcAXwV2pjtY8LVVtaI/APAEupkyfgasD7wCuBD4SV/Kk+gC8tHAh6rqpqntJEmSFhoD8xiqqhuALwJfTLLlwE0vpgu5+9FNLXcF8GO6WStm7vvBJEuAl/WX84BXVNWh/SYr6HqaX0bXc3w1cBqwa1Vd02/zObqQfv3ctFCSJGlyGJjHXFVdNPBzAe/qL637vJduLubZbltJF7hb93euZUmSpJ5jmCVJkqQGA7MkSZLUYGCWJEmSGgzMkiRJUoOBWZIkSWowMEuSJEkNBmZJkiSpwcAsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSg4FZkiRJajAwS5IkSQ0GZkmSJKnBwCxJkiQ1GJglSZKkBgOzJEmS1GBgliRJkhoMzJIkSVJDqmrUNWiKJbkY+PU87vIOwO/ncX/zaZrbBrZv0tm+yTXNbQPbN+nmu313q6othlcamDVVkiyrqqWjrmMuTHPbwPZNOts3uaa5bWD7Jt24tM8hGZIkSVKDgVmSJElqMDBr2hwx6gLm0DS3DWzfpLN9k2ua2wa2b9KNRfscwyxJkiQ12MMsSZIkNRiYJUmSpAYDsyRJktRgYJYkSZIaDMySJElSw/8PzlTzYC5Gj0kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvxZOsJf-JpX"
   },
   "source": [
    "#### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "JdJNEJ7V-JpY"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIZTeEIj-JpY",
    "outputId": "53b9949a-7523-41e6-844c-9b630ee7de30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 37.09\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "uXjxIW5m-JpY"
   },
   "outputs": [],
   "source": [
    "sen_list = [\n",
    "'모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .',\n",
    "'미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?',\n",
    "'은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요',\n",
    "'아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?',\n",
    "'부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .',\n",
    "'변기 가 막히 었 습니다 .',\n",
    "'그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?',\n",
    "'비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .',\n",
    "'속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다',\n",
    "'문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .',\n",
    "'이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .',\n",
    "'이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .',\n",
    "'통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .',\n",
    "'이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .',\n",
    "'요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다 ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "hGj-fPAS-JpZ",
    "outputId": "79eff127-53bf-4c8b-a538-33c25d460895",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .</td>\n",
       "      <td>i 'm a bag , gels , gels , gels , gels , gels , and liquids . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?</td>\n",
       "      <td>can you give me a &lt;unk&gt; for the &lt;unk&gt; to &lt;unk&gt; , but i 'm afraid i 'm going to get a &lt;unk&gt; to go to &lt;unk&gt; , but i 'm going to get a &lt;unk&gt; for the &lt;unk&gt; to &lt;unk&gt; to get a &lt;unk&gt; , but i 'm going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요</td>\n",
       "      <td>i need money . the &lt;unk&gt; is not too good . the bank is not too far to get a full . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?</td>\n",
       "      <td>are you going to have to have to have to have to have to have to have a form . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .</td>\n",
       "      <td>&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; to &lt;unk&gt; &lt;unk&gt; to &lt;unk&gt; &lt;unk&gt; to &lt;unk&gt; &lt;unk&gt; to &lt;unk&gt; &lt;unk&gt; from &lt;unk&gt; &lt;unk&gt; from &lt;unk&gt; &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>변기 가 막히 었 습니다 .</td>\n",
       "      <td>it 's a traffic jam . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?</td>\n",
       "      <td>how much is it in a little ? &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .</td>\n",
       "      <td>i 'd like to have a department store . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다</td>\n",
       "      <td>i do n't feel good to be a good time to go in the morning . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .</td>\n",
       "      <td>i 'm afraid the profit is &lt;unk&gt; from the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .</td>\n",
       "      <td>i 'll take a few time to go to a few time to go for a few time to go to a few time to go to a few time to take a few time to go to a few time to go to a few time to take a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .</td>\n",
       "      <td>i have a full name of the job . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .</td>\n",
       "      <td>the &lt;unk&gt; &lt;unk&gt; the &lt;unk&gt; &lt;unk&gt; the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .</td>\n",
       "      <td>i need to have a &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다</td>\n",
       "      <td>the program is &lt;unk&gt; for the end . &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Korean                                                                                                                                                                                           English\n",
       "0                                  모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .                                                                                                                               i 'm a bag , gels , gels , gels , gels , gels , and liquids . <eos>\n",
       "1                 미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?  can you give me a <unk> for the <unk> to <unk> , but i 'm afraid i 'm going to get a <unk> to go to <unk> , but i 'm going to get a <unk> for the <unk> to <unk> to get a <unk> , but i 'm going\n",
       "2                                            은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요                                                                                                          i need money . the <unk> is not too good . the bank is not too far to get a full . <eos>\n",
       "3                         아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?                                                                                                              are you going to have to have to have to have to have to have to have a form . <eos>\n",
       "4                            부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .                                                                     <unk> <unk> <unk> <unk> to <unk> <unk> to <unk> <unk> to <unk> <unk> to <unk> <unk> from <unk> <unk> from <unk> <unk> . <eos>\n",
       "5                                                                           변기 가 막히 었 습니다 .                                                                                                                                                                       it 's a traffic jam . <eos>\n",
       "6                                         그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?                                                                                                                                                                how much is it in a little ? <eos>\n",
       "7                                              비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .                                                                                                                                                      i 'd like to have a department store . <eos>\n",
       "8                                                    속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다                                                                                                                                 i do n't feel good to be a good time to go in the morning . <eos>\n",
       "9                                                      문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .                                                                                                                                            i 'm afraid the profit is <unk> from the <unk> . <eos>\n",
       "10                                                        이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .                       i 'll take a few time to go to a few time to go for a few time to go to a few time to go to a few time to take a few time to go to a few time to go to a few time to take a\n",
       "11                                                       이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .                                                                                                                                                             i have a full name of the job . <eos>\n",
       "12                     통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .                                                      the <unk> <unk> the <unk> <unk> the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> . <eos>\n",
       "13  이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .                                                                                                   i need to have a <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> . <eos>\n",
       "14                                                             요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다                                                                                                                                                           the program is <unk> for the end . <eos>"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in sen_list:\n",
    "  translation, attention = translate_sentence(s, SRC, TRG, model, device)\n",
    "  pred = ' '.join(translation)\n",
    "  final = {'Korean': s, 'English': pred}\n",
    "  df=df.append(final, ignore_index=True)\n",
    "\n",
    "\n",
    "pd.set_option('max_colwidth', 200)\n",
    "df = df[['Korean','English']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKFZBA7KgmHw"
   },
   "source": [
    "# 4. Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "eh6uIsYsTn8f",
    "outputId": "35355e05-71db-4b93-d122-aa5a787c2eed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\\n    (train_data, valid_data, test_data), \\n     batch_size = BATCH_SIZE,\\n     device = device)\\n'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     sort=False, device = device)\n",
    "\"\"\"\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "niBbMPWxgmIA"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "z7pjq26VgmIB"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "oAUvnG-egmID"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "       \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "h76w4dvSgmIE"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "_4fdbDMhgmIE"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "                            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "                \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "rkw_3nP1gmIE"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "sbO-H-BJgmIF"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "nX_BrkuVgmIF"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "kei2KoudgmIF"
   },
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpFs_465gmIG",
    "outputId": "4189f99b-487c-4280-a353-f0cc2be8d447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 15,522,567 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "OggFbJtCgmIH"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "5eZpeo0tgmIH"
   },
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "aQz15u16gmIH"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "PA6UHaEQgmII"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "h2RUCo_WgmII"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "NU_98979gmII"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "bukWpxk2gmII"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPcGNZhjgmIJ",
    "outputId": "c95ba6ab-9318-404d-c781-44ab0a40478d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 22s\n",
      "\tTrain Loss: 2.595 | Train PPL:  13.402\n",
      "\t Val. Loss: 1.679 |  Val. PPL:   5.363\n",
      "Epoch: 02 | Time: 2m 21s\n",
      "\tTrain Loss: 1.534 | Train PPL:   4.635\n",
      "\t Val. Loss: 1.344 |  Val. PPL:   3.833\n",
      "Epoch: 03 | Time: 2m 14s\n",
      "\tTrain Loss: 1.232 | Train PPL:   3.428\n",
      "\t Val. Loss: 1.195 |  Val. PPL:   3.305\n",
      "Epoch: 04 | Time: 2m 16s\n",
      "\tTrain Loss: 1.061 | Train PPL:   2.888\n",
      "\t Val. Loss: 1.117 |  Val. PPL:   3.056\n",
      "Epoch: 05 | Time: 2m 20s\n",
      "\tTrain Loss: 0.950 | Train PPL:   2.586\n",
      "\t Val. Loss: 1.059 |  Val. PPL:   2.883\n",
      "Epoch: 06 | Time: 2m 33s\n",
      "\tTrain Loss: 0.870 | Train PPL:   2.387\n",
      "\t Val. Loss: 1.025 |  Val. PPL:   2.787\n",
      "Epoch: 07 | Time: 2m 23s\n",
      "\tTrain Loss: 0.807 | Train PPL:   2.242\n",
      "\t Val. Loss: 0.993 |  Val. PPL:   2.700\n",
      "Epoch: 08 | Time: 2m 16s\n",
      "\tTrain Loss: 0.757 | Train PPL:   2.132\n",
      "\t Val. Loss: 0.976 |  Val. PPL:   2.654\n",
      "Epoch: 09 | Time: 2m 24s\n",
      "\tTrain Loss: 0.717 | Train PPL:   2.048\n",
      "\t Val. Loss: 0.956 |  Val. PPL:   2.601\n",
      "Epoch: 10 | Time: 2m 21s\n",
      "\tTrain Loss: 0.682 | Train PPL:   1.978\n",
      "\t Val. Loss: 0.944 |  Val. PPL:   2.571\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxMfnagXgmIJ",
    "outputId": "2ac4d38c-8ea0-4a23-8bb3-47487a5c4bf9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.947 | Test PPL:   2.577 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut6-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vqODZLzCgmIK"
   },
   "outputs": [],
   "source": [
    "from korean_romanizer.romanizer import Romanizer\n",
    "\n",
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        tokens = [token for token in sentence.split()]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    #trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    trg_tokens =[]\n",
    "    ti = 0\n",
    "    for i in trg_indexes:\n",
    "      if trg_field.vocab.itos[i] != '<unk>':\n",
    "        trg_tokens.append(trg_field.vocab.itos[i])\n",
    "      else:\n",
    "        try:\n",
    "          trg_tokens.append(Romanizer(tokens[ti]).romanize())\n",
    "        except:\n",
    "          continue\n",
    "      ti+=1\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WPWMqsF6gmIK"
   },
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
    "    \n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    \n",
    "    for i in range(n_heads):\n",
    "        \n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        \n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='bone')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                           rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZeNX0qV2gmIK",
    "outputId": "15d6ed9e-5c83-471b-ae97-2ff7811e6d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['휴가', '중', '이', 'ㅂ니다', '.']\n",
      "trg = ['he', 'is', 'off', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 8\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "trg = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57KBp9xFgmIL"
   },
   "source": [
    "Our translation looks pretty good, although our model changes *is walking by* to *walks by*. The meaning is still the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnSE0XvegmIL",
    "outputId": "833ab818-6218-44f5-8212-4c8769a8b4b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['i', \"'m\", 'on', 'vacation', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d8ytVWU-gmIL",
    "outputId": "00394880-a86b-41c7-ff1d-994974cd9526",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 55092 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 44032 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 51473 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 51060 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12610 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 45768 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 45796 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 55092 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 44032 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 51473 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 51060 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 12610 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 45768 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 45796 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1800 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "_bj5tsXxgmIO"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Romanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vqODZLzCgmIK"
   },
   "outputs": [],
   "source": [
    "from korean_romanizer.romanizer import Romanizer\n",
    "\n",
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        tokens = [token for token in sentence.split()]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    #trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    ## Romanize Start\n",
    "    trg_tokens =[]\n",
    "    ti = 0\n",
    "    for i in trg_indexes:\n",
    "      if trg_field.vocab.itos[i] != '<unk>':\n",
    "        trg_tokens.append(trg_field.vocab.itos[i])\n",
    "      else:\n",
    "        try:\n",
    "          trg_tokens.append(Romanizer(tokens[ti]).romanize())\n",
    "        except:\n",
    "          continue\n",
    "      ti+=1\n",
    "    ## Romanize End\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reverse Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qNh3z4kq-JpD"
   },
   "outputs": [],
   "source": [
    "def tokenize_ko(text):\n",
    "    return [tok for tok in text.split()][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok for tok in text.split()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
